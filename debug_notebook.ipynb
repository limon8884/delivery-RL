{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src_new.reinforcement.lunar_lander import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_envs = 1\n",
    "trajectory_lenght = 2048\n",
    "batch_size = 64\n",
    "num_epochs_per_traj = 10\n",
    "total_iters = 250000\n",
    "device = None\n",
    "env_name = 'LunarLander-v2'\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_actions = torch.randint(0, 4, size=(trajectory_lenght + 1, n_envs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_logger = Logger()\n",
    "pred_env = GymEnv(gym_name=env_name)\n",
    "ac = GymActorCritic(input_dim=pred_env.state_dim, action_dim=pred_env.action_dim, device=device, debug_tens=global_actions)\n",
    "# opt = torch.optim.Adam(ac.parameters(), lr=3e-4, eps=1e-5)\n",
    "# ppo = PPO(actor_critic=ac, optimizer=opt, device=device, logger=logger)\n",
    "pred_runner = Runner(environment=pred_env, actor_critic=ac,\n",
    "                n_envs=n_envs, trajectory_lenght=trajectory_lenght)\n",
    "pred_runner.reset(list(range(n_envs)))\n",
    "pred_inference_logger = InferenceMetricsRunner(runner=pred_runner, logger=pred_logger)\n",
    "pred_gae = GAE()\n",
    "pred_normalizer = RewardNormalizer()\n",
    "buffer = Buffer(gae=pred_gae, reward_normalizer=pred_normalizer, device=device)\n",
    "pred_sampler = TrajectorySampler(pred_runner, buffer, num_epochs_per_traj=num_epochs_per_traj, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sample = pred_sampler.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lunar_lander_debug_notebook import (\n",
    "    Summaries,\n",
    "    Normalize,\n",
    "    Policy,\n",
    "    AsArray,\n",
    "    EnvRunner,\n",
    "    GAE as GAE_OLD,\n",
    "    TrajectorySampler as TrajectorySampler_OLD,\n",
    "    NormalizeAdvantages\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "tgt_env = Normalize(Summaries(gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")), obs=False, ret=True)\n",
    "tgt_env.reset(seed=0)\n",
    "\n",
    "policy = Policy(model=ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_runner_transforms = [AsArray(), GAE_OLD(policy)]\n",
    "tgt_runner = EnvRunner(tgt_env, policy, trajectory_lenght, transforms=tgt_runner_transforms)\n",
    "tgt_runner.reset(seed=0)\n",
    "tgt_sampler_transforms = [NormalizeAdvantages()]\n",
    "ac._debug_iter = 0\n",
    "tgt_sampler = TrajectorySampler_OLD(tgt_runner, num_epochs=num_epochs_per_traj,\n",
    "                            num_minibatches=32,\n",
    "                            transforms=tgt_sampler_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_sample = tgt_sampler.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions [0 3 1 0 3 3 3 3 1 3]\n",
      "log_probs [-1.41075122 -1.32259488 -1.43524849 -1.41357636 -1.31906962 -1.31453574\n",
      " -1.30926061 -1.30473089 -1.44244266 -1.30267704]\n",
      "values [0.07636027 0.08223443 0.08993445 0.09443133 0.10070428 0.10852748\n",
      " 0.11693294 0.12532517 0.13368753 0.13942483]\n",
      "observations [[ 0.00570612  1.3990337   0.5779653  -0.5282997  -0.0066053  -0.13091765\n",
      "   0.          0.        ]\n",
      " [ 0.01141253  1.3865713   0.57718486 -0.55392617 -0.01307467 -0.1293994\n",
      "   0.          0.        ]\n",
      " [ 0.01719465  1.3735025   0.5866798  -0.5809347  -0.02144145 -0.16735117\n",
      "   0.          0.        ]\n",
      " [ 0.02288361  1.3598258   0.57497466 -0.6079506  -0.02745714 -0.12032503\n",
      "   0.          0.        ]\n",
      " [ 0.02857265  1.3455497   0.5749912  -0.6346211  -0.03347334 -0.12033501\n",
      "   0.          0.        ]\n",
      " [ 0.03433599  1.3306733   0.5843054  -0.6613757  -0.04135226 -0.15759297\n",
      "   0.          0.        ]\n",
      " [ 0.04019766  1.3152038   0.5965973  -0.6878521  -0.05168391 -0.20665224\n",
      "   0.          0.        ]\n",
      " [ 0.04614439  1.2991339   0.6072491  -0.71469504 -0.06414291 -0.2492028\n",
      "   0.          0.        ]\n",
      " [ 0.05216599  1.282464    0.6166166  -0.7415718  -0.07847096 -0.28658766\n",
      "   0.          0.        ]\n",
      " [ 0.05811787  1.2651958   0.6078506  -0.7681878  -0.09103426 -0.25128907\n",
      "   0.          0.        ]]\n",
      "rewards [-10.          -2.01016642  -0.35375093  -0.88117151  -1.08641395\n",
      "  -0.93604368  -0.74526061  -0.61705467  -0.24291905  -0.46842746]\n",
      "resets [False False False False False False False False False False]\n",
      "advantages [-6.0752913  -1.83802321 -1.09978192 -1.09411097 -0.84027192 -0.47432009\n",
      " -0.15633338  0.09188444  0.29542501  0.33674577]\n",
      "value_targets [-16.83003296  -7.2664578   -5.59355996  -5.57627153  -4.99743209\n",
      "  -4.16415764  -3.43849244  -2.8702131   -2.40273893  -2.30379735]\n"
     ]
    }
   ],
   "source": [
    "for k in tgt_sample.keys():\n",
    "    print(k, tgt_sample[k][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "advantages tensor([-6.0276, -1.8237, -1.0912, -1.0856, -0.8337, -0.4706, -0.1551,  0.0912,\n",
      "         0.2931,  0.3341])\n",
      "log_probs_chosen tensor([-1.4108, -1.3226, -1.4352, -1.4136, -1.3191, -1.3145, -1.3093, -1.3047,\n",
      "        -1.4424, -1.3027])\n",
      "values tensor([0.0764, 0.0822, 0.0899, 0.0944, 0.1007, 0.1085, 0.1169, 0.1253, 0.1337,\n",
      "        0.1394])\n",
      "states [<src_new.reinforcement.lunar_lander.GymState object at 0x7f7fe1b35d20>, <src_new.reinforcement.lunar_lander.GymState object at 0x7f80bc0d22f0>, <src_new.reinforcement.lunar_lander.GymState object at 0x7f7fe1b36e90>, <src_new.reinforcement.lunar_lander.GymState object at 0x7f7fe1b36d10>, <src_new.reinforcement.lunar_lander.GymState object at 0x7f7fe1b35db0>, <src_new.reinforcement.lunar_lander.GymState object at 0x7f7fe1cefe80>, <src_new.reinforcement.lunar_lander.GymState object at 0x7f7fe1cefc10>, <src_new.reinforcement.lunar_lander.GymState object at 0x7f7fe1cefdc0>, <src_new.reinforcement.lunar_lander.GymState object at 0x7f7fe1cefd30>, <src_new.reinforcement.lunar_lander.GymState object at 0x7f7fe1cef970>]\n",
      "actions_chosen tensor([0, 3, 1, 0, 3, 3, 3, 3, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "for k in pred_sample.keys():\n",
    "    print(k, pred_sample[k][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac._debug_iter = 0\n",
    "tgt_traj = tgt_runner.get_next()\n",
    "ac._debug_iter = 0\n",
    "pred_traj = pred_runner.run()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_normalizer([pred_traj])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "advs = GAE()(pred_traj)\n",
    "for i in range(trajectory_lenght):\n",
    "    if abs(tgt_traj['values'][i] - pred_traj.values[i]) > 1e-7:\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-16.90639323,  -7.34869223,  -5.68349441,  -5.67070286,\n",
       "        -5.09813637])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_traj['advantages'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-16.906734613453963,\n",
       " -7.349055211198873,\n",
       " -5.683664658190591,\n",
       " -5.67084857285135,\n",
       " -5.098227355989695]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "advs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_sampler = make_ppo_runner(tgt_env, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_sample = tgt_sampler.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 3, 2, 0, 1, 2, 0, 0, 0, 0, 1, 1, 1, 2, 2, 1, 0, 0, 3, 0, 3, 3, 0, 1,\n",
       "        3, 1, 3, 1, 3, 0, 3, 2, 1, 3, 1, 3, 1, 0, 2, 0, 0, 3, 2, 1, 0, 2, 2, 3,\n",
       "        2, 0, 1, 3, 1, 0, 0, 2, 3, 2, 1, 0, 3, 0, 3, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_sample['actions_chosen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 3, 1, 0, 3, 3, 3, 3, 1, 3, 1, 2, 0, 3, 2, 0, 0, 0, 2, 1, 2, 3,\n",
       "       3, 2, 0, 1, 1, 1, 1, 0, 1, 0, 3, 0, 3, 1, 2, 3, 3, 0, 2, 3, 0, 1,\n",
       "       3, 1, 3, 3, 2, 3, 0, 1, 1, 1, 3, 0, 3, 2, 0, 3, 3, 2, 3, 2])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_sample['actions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
