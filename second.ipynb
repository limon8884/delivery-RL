{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from utils import *\n",
    "from dispatch.utils import *\n",
    "\n",
    "import dispatch\n",
    "import importlib\n",
    "# importlib.reload(dispatch)\n",
    "\n",
    "from utils import *\n",
    "# from dispatch.network import ScoringNet, PointEncoder, PositionalEncoder\n",
    "from networks.encoders import PointEncoder\n",
    "from dispatch.dispatch import Dispatch\n",
    "from simulator.base_simulator import BaseSimulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self) -> None:\n",
    "        self.env = gym.make('CartPole-v0')\n",
    "        self.state = None\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.env.reset()\n",
    "\n",
    "    def step(self, action):\n",
    "        '''\n",
    "        returns: reward\n",
    "        '''\n",
    "        state, reward, done, info = self.env.step(action)\n",
    "        self.state = state\n",
    "        if done:\n",
    "            self.reset()\n",
    "            return 0\n",
    "        return reward\n",
    "\n",
    "    def get_state(self):\n",
    "        '''\n",
    "        returns the copy of state via inherence\n",
    "        '''\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def estimate_value(self, state):\n",
    "        inp = torch.tensor(state, dtype=torch.float32)\n",
    "        return self.model(inp)[1]\n",
    "\n",
    "    def act(self, state):\n",
    "        inp = torch.tensor(state, dtype=torch.float32)\n",
    "        actor, critic = self.model(inp)\n",
    "    \n",
    "        # probs = nn.functional.softmax(a, dim=-1).detach().numpy()\n",
    "        # actions = np.array([np.random.choice(6, p=probs[i]) for i in range(len(inputs))])\n",
    "        action_probs = nn.functional.softmax(actor, dim=-1)\n",
    "        action = torch.distributions.Categorical(action_probs).sample()\n",
    "        # logits = torch.gather(a, dim=1, index=torch.tensor(actions).unsqueeze(-1)).squeeze()\n",
    "\n",
    "        log_softmaxed = nn.functional.log_softmax(actor, dim=-1)\n",
    "\n",
    "        # print(log_softmaxed[action.unsqueeze(-1)])\n",
    "        log_probs = torch.gather(log_softmaxed, dim=-1, index=action.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        d = {}\n",
    "        d['actions'] = action\n",
    "        d['logits'] = actor\n",
    "        d['log_probs'] = log_probs\n",
    "        d['values'] = critic\n",
    "\n",
    "        return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModel(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.body = nn.Sequential(\n",
    "            nn.Linear(4, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 5),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.actor = nn.Linear(5, 2)\n",
    "        self.critic = nn.Linear(5, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.body(x)\n",
    "        return self.actor(x), self.critic(x).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-02-10 21:03:31,914] Making new env: CartPole-v0\n",
      "c:\\Users\\Danill\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gym\\envs\\registration.py:17: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TestModel()\n",
    "policy = Policy(model)\n",
    "env = Environment()\n",
    "\n",
    "d = policy.act(env.get_state())\n",
    "env.step(d['actions'].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from collections import defaultdict\n",
    "\n",
    "class SessionCreater:\n",
    "    def __init__(self, env, policy, n_steps=4, n_sessions=8, mode='parallel') -> None:\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "        self.n_steps = n_steps\n",
    "        self.n_sessions = n_sessions\n",
    "        self.mode = mode\n",
    "\n",
    "    def generate_session(self):\n",
    "        '''\n",
    "        output trajectory: dict with keys:\n",
    "        -states\n",
    "        -actions\n",
    "        -rewards\n",
    "\n",
    "        -logits\n",
    "        -log_probs\n",
    "        \n",
    "        -values\n",
    "        -advantages\n",
    "        '''\n",
    "        trajectory = defaultdict(list)\n",
    "        self.env.reset()\n",
    "\n",
    "        for i in range(self.n_steps):\n",
    "            trajectory['states'].append(self.env.get_state())\n",
    "            act = self.policy.act(self.env.get_state())\n",
    "\n",
    "            trajectory['actions'].append(act['actions'])\n",
    "            trajectory['logits'].append(act['logits'])\n",
    "            trajectory['log_probs'].append(act['log_probs'])\n",
    "            trajectory['values'].append(act['values'])\n",
    "\n",
    "            reward = self.env.step(trajectory[\"actions\"][-1].item())\n",
    "            trajectory['rewards'].append(reward)\n",
    "\n",
    "        trajectory['last_state'] = {\n",
    "            'state': self.env.get_state(),\n",
    "            'value': self.policy.estimate_value(self.env.get_state())\n",
    "        }\n",
    "\n",
    "        return trajectory\n",
    "\n",
    "    def parallel_generate_sessions(self):\n",
    "        return Parallel(n_jobs=-1)(delayed(self.generate_session)() for i in range(self.n_sessions))\n",
    "    \n",
    "    def sequential_generate_sessions(self):\n",
    "        return [self.generate_session() for i in range(self.n_sessions)]\n",
    "    \n",
    "    def __call__(self):\n",
    "        if self.mode == 'parallel':\n",
    "            return self.parallel_generate_sessions()\n",
    "        return self.sequential_generate_sessions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_estimated_cumulative_rewards(rewards, last_state_value, gamma=0.99):\n",
    "    '''\n",
    "    input: \n",
    "    -rewards: List[float] \n",
    "    -last_state_q_value: float \n",
    "    output: \n",
    "    -cum_rewards: List[float]\n",
    "    '''\n",
    "    reverse_rewards = [last_state_q_value]\n",
    "    n_steps = len(rewards)\n",
    "    for i in range(n_steps, 0, -1):\n",
    "        reverse_rewards.append(reverse_rewards[-1] * gamma + rewards[i - 1])\n",
    "    return reverse_rewards[1:][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComputeValueTargets:\n",
    "    def __init__(self, gamma=0.99):\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def compute_cumutitive_rewards(self, rewards, last_state_value):\n",
    "        reverse_rewards = [last_state_value]\n",
    "        n_steps = len(rewards)\n",
    "        for i in range(n_steps, 0, -1):\n",
    "            reverse_rewards.append(reverse_rewards[-1] * self.gamma + rewards[i - 1])\n",
    "        return reverse_rewards[1:][::-1]\n",
    "    \n",
    "    def __call__(self, session):\n",
    "        for trajectory in session:\n",
    "            rewards = trajectory['rewards']\n",
    "            last_state_value = trajectory['last_state']['value']\n",
    "            trajectory['targets'] = self.compute_cumutitive_rewards(rewards, last_state_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'states': [array([-0.03555699, -0.02499039,  0.00473004,  0.01856952]),\n",
       "              array([-0.03605679, -0.22017985,  0.00510143,  0.31274107]),\n",
       "              array([-0.04046039, -0.02513094,  0.01135625,  0.02167133]),\n",
       "              array([-0.04096301,  0.16982632,  0.01178968, -0.26740704])],\n",
       "             'actions': [tensor(0), tensor(1), tensor(1), tensor(0)],\n",
       "             'logits': [tensor([-4.1165e-01, -1.1660e-04], grad_fn=<AddBackward0>),\n",
       "              tensor([-0.4294, -0.0023], grad_fn=<AddBackward0>),\n",
       "              tensor([-0.4124, -0.0007], grad_fn=<AddBackward0>),\n",
       "              tensor([-0.3986, -0.0016], grad_fn=<AddBackward0>)],\n",
       "             'log_probs': [tensor(-0.9199, grad_fn=<SqueezeBackward1>),\n",
       "              tensor(-0.5022, grad_fn=<SqueezeBackward1>),\n",
       "              tensor(-0.5083, grad_fn=<SqueezeBackward1>),\n",
       "              tensor(-0.9113, grad_fn=<SqueezeBackward1>)],\n",
       "             'values': [tensor(0.2142, grad_fn=<SqueezeBackward1>),\n",
       "              tensor(0.2384, grad_fn=<SqueezeBackward1>),\n",
       "              tensor(0.2152, grad_fn=<SqueezeBackward1>),\n",
       "              tensor(0.1946, grad_fn=<SqueezeBackward1>)],\n",
       "             'rewards': [1.0, 1.0, 1.0, 1.0],\n",
       "             'last_state': {'state': array([-0.03756648, -0.02546189,  0.00644154,  0.028971  ]),\n",
       "              'value': tensor(0.2163, grad_fn=<SqueezeBackward1>)},\n",
       "             'targets': [tensor(4.1482, grad_fn=<AddBackward0>),\n",
       "              tensor(3.1800, grad_fn=<AddBackward0>),\n",
       "              tensor(2.2020, grad_fn=<AddBackward0>),\n",
       "              tensor(1.2141, grad_fn=<AddBackward0>)]})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvt = ComputeValueTargets()\n",
    "s = SessionCreater(env, policy, mode='def')\n",
    "trs = s()\n",
    "cvt(trs)\n",
    "trs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_time_batch(sessions):\n",
    "    keys = [k for k in sessions[0].keys() if k != 'last_state']\n",
    "    result = {}\n",
    "    for key in keys:\n",
    "        result[key] = torch.hstack([s[key] for s in sessions])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-125-9414e763290c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmerge_time_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-123-6cfbcedb6709>\u001b[0m in \u001b[0;36mmerge_time_batch\u001b[1;34m(sessions)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got list"
     ]
    }
   ],
   "source": [
    "merge_time_batch(trs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sessions_to_tensors(trajectory):\n",
    "    # trajectory['targets'] = compute_estimated_cumulative_rewards(trajectory['rewards'], trajectory['last_state']['q_value'])\n",
    "    for k, v in trajectory.items():\n",
    "        pass\n",
    "\n",
    "def compute_losses():\n",
    "    '''\n",
    "    input:\n",
    "    -\n",
    "    output: \n",
    "    -policy_loss \n",
    "    -value_loss (V = G) \n",
    "    -entropy_loss\n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a35fb83ac8e6ec7d775c8e4493051254ee64ddb3600bc3fef060a4811657ed17"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
