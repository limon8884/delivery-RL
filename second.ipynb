{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from utils import *\n",
    "from dispatch.utils import *\n",
    "\n",
    "import dispatch\n",
    "import importlib\n",
    "# importlib.reload(dispatch)\n",
    "\n",
    "from utils import *\n",
    "# from dispatch.network import ScoringNet, PointEncoder, PositionalEncoder\n",
    "from networks.encoders import PointEncoder\n",
    "from dispatch.dispatch import Dispatch\n",
    "from simulator.base_simulator import BaseSimulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self) -> None:\n",
    "        self.env = gym.make('CartPole-v0')\n",
    "        self.state = None\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.env.reset()\n",
    "\n",
    "    def step(self, action):\n",
    "        '''\n",
    "        returns: reward\n",
    "        '''\n",
    "        state, reward, done, info = self.env.step(action)\n",
    "        self.state = state\n",
    "        if done:\n",
    "            self.reset()\n",
    "            return 0\n",
    "        return reward\n",
    "\n",
    "    def get_state(self):\n",
    "        '''\n",
    "        returns the copy of state via inherence\n",
    "        '''\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def estimate_value(self, state):\n",
    "        inp = torch.tensor(state, dtype=torch.float32)\n",
    "        return self.model(inp)[1]\n",
    "\n",
    "    def act(self, state):\n",
    "        inp = torch.tensor(state, dtype=torch.float32)\n",
    "        actor, critic = self.model(inp)\n",
    "    \n",
    "        # probs = nn.functional.softmax(a, dim=-1).detach().numpy()\n",
    "        # actions = np.array([np.random.choice(6, p=probs[i]) for i in range(len(inputs))])\n",
    "        action_probs = nn.functional.softmax(actor, dim=-1)\n",
    "        action = torch.distributions.Categorical(action_probs).sample()\n",
    "        # logits = torch.gather(a, dim=1, index=torch.tensor(actions).unsqueeze(-1)).squeeze()\n",
    "\n",
    "        log_softmaxed = nn.functional.log_softmax(actor, dim=-1)\n",
    "\n",
    "        # print(log_softmaxed[action.unsqueeze(-1)])\n",
    "        log_probs = torch.gather(log_softmaxed, dim=-1, index=action.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        d = {}\n",
    "        d['actions'] = action\n",
    "        d['logits'] = actor\n",
    "        d['log_probs'] = log_probs\n",
    "        d['values'] = critic\n",
    "\n",
    "        return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModel(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.body = nn.Sequential(\n",
    "            nn.Linear(4, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 5),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.actor = nn.Linear(5, 2)\n",
    "        self.critic = nn.Linear(5, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.body(x)\n",
    "        return self.actor(x), self.critic(x).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-02-10 21:03:31,914] Making new env: CartPole-v0\n",
      "c:\\Users\\Danill\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gym\\envs\\registration.py:17: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TestModel()\n",
    "policy = Policy(model)\n",
    "env = Environment()\n",
    "\n",
    "d = policy.act(env.get_state())\n",
    "env.step(d['actions'].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from collections import defaultdict\n",
    "\n",
    "class SessionCreater:\n",
    "    def __init__(self, env, policy, n_steps=4, n_sessions=8, mode='parallel') -> None:\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "        self.n_steps = n_steps\n",
    "        self.n_sessions = n_sessions\n",
    "        self.mode = mode\n",
    "\n",
    "    def generate_session(self):\n",
    "        '''\n",
    "        output trajectory: dict with keys:\n",
    "        -states\n",
    "        -actions\n",
    "        -rewards\n",
    "\n",
    "        -logits\n",
    "        -log_probs\n",
    "        \n",
    "        -values\n",
    "        -advantages\n",
    "        '''\n",
    "        trajectory = defaultdict(list)\n",
    "        self.env.reset()\n",
    "\n",
    "        for i in range(self.n_steps):\n",
    "            trajectory['states'].append(self.env.get_state())\n",
    "            act = self.policy.act(self.env.get_state())\n",
    "\n",
    "            trajectory['actions'].append(act['actions'])\n",
    "            trajectory['logits'].append(act['logits'])\n",
    "            trajectory['log_probs'].append(act['log_probs'])\n",
    "            trajectory['values'].append(act['values'])\n",
    "\n",
    "            reward = self.env.step(trajectory[\"actions\"][-1].item())\n",
    "            trajectory['rewards'].append(reward)\n",
    "\n",
    "        trajectory['last_state'] = {\n",
    "            'state': self.env.get_state(),\n",
    "            'value': self.policy.estimate_value(self.env.get_state())\n",
    "        }\n",
    "\n",
    "        return trajectory\n",
    "\n",
    "    def parallel_generate_sessions(self):\n",
    "        return Parallel(n_jobs=-1)(delayed(self.generate_session)() for i in range(self.n_sessions))\n",
    "    \n",
    "    def sequential_generate_sessions(self):\n",
    "        return [self.generate_session() for i in range(self.n_sessions)]\n",
    "    \n",
    "    def __call__(self):\n",
    "        if self.mode == 'parallel':\n",
    "            return self.parallel_generate_sessions()\n",
    "        return self.sequential_generate_sessions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_estimated_cumulative_rewards(rewards, last_state_value, gamma=0.99):\n",
    "    '''\n",
    "    input: \n",
    "    -rewards: List[float] \n",
    "    -last_state_q_value: float \n",
    "    output: \n",
    "    -cum_rewards: List[float]\n",
    "    '''\n",
    "    reverse_rewards = [last_state_q_value]\n",
    "    n_steps = len(rewards)\n",
    "    for i in range(n_steps, 0, -1):\n",
    "        reverse_rewards.append(reverse_rewards[-1] * gamma + rewards[i - 1])\n",
    "    return reverse_rewards[1:][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComputeValueTargets:\n",
    "    def __init__(self, gamma=0.99):\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def compute_cumutitive_rewards(self, rewards, last_state_value):\n",
    "        reverse_rewards = [last_state_value]\n",
    "        n_steps = len(rewards)\n",
    "        for i in range(n_steps, 0, -1):\n",
    "            reverse_rewards.append(reverse_rewards[-1] * self.gamma + rewards[i - 1])\n",
    "        return reverse_rewards[1:][::-1]\n",
    "    \n",
    "    def __call__(self, session):\n",
    "        for trajectory in session:\n",
    "            rewards = trajectory['rewards']\n",
    "            last_state_value = trajectory['last_state']['value']\n",
    "            trajectory['targets'] = self.compute_cumutitive_rewards(rewards, last_state_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'states': [array([-0.03555699, -0.02499039,  0.00473004,  0.01856952]),\n",
       "              array([-0.03605679, -0.22017985,  0.00510143,  0.31274107]),\n",
       "              array([-0.04046039, -0.02513094,  0.01135625,  0.02167133]),\n",
       "              array([-0.04096301,  0.16982632,  0.01178968, -0.26740704])],\n",
       "             'actions': [tensor(0), tensor(1), tensor(1), tensor(0)],\n",
       "             'logits': [tensor([-4.1165e-01, -1.1660e-04], grad_fn=<AddBackward0>),\n",
       "              tensor([-0.4294, -0.0023], grad_fn=<AddBackward0>),\n",
       "              tensor([-0.4124, -0.0007], grad_fn=<AddBackward0>),\n",
       "              tensor([-0.3986, -0.0016], grad_fn=<AddBackward0>)],\n",
       "             'log_probs': [tensor(-0.9199, grad_fn=<SqueezeBackward1>),\n",
       "              tensor(-0.5022, grad_fn=<SqueezeBackward1>),\n",
       "              tensor(-0.5083, grad_fn=<SqueezeBackward1>),\n",
       "              tensor(-0.9113, grad_fn=<SqueezeBackward1>)],\n",
       "             'values': [tensor(0.2142, grad_fn=<SqueezeBackward1>),\n",
       "              tensor(0.2384, grad_fn=<SqueezeBackward1>),\n",
       "              tensor(0.2152, grad_fn=<SqueezeBackward1>),\n",
       "              tensor(0.1946, grad_fn=<SqueezeBackward1>)],\n",
       "             'rewards': [1.0, 1.0, 1.0, 1.0],\n",
       "             'last_state': {'state': array([-0.03756648, -0.02546189,  0.00644154,  0.028971  ]),\n",
       "              'value': tensor(0.2163, grad_fn=<SqueezeBackward1>)},\n",
       "             'targets': [tensor(4.1482, grad_fn=<AddBackward0>),\n",
       "              tensor(3.1800, grad_fn=<AddBackward0>),\n",
       "              tensor(2.2020, grad_fn=<AddBackward0>),\n",
       "              tensor(1.2141, grad_fn=<AddBackward0>)]})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvt = ComputeValueTargets()\n",
    "s = SessionCreater(env, policy, mode='def')\n",
    "trs = s()\n",
    "cvt(trs)\n",
    "trs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_time_batch(sessions):\n",
    "    keys = [k for k in sessions[0].keys() if k != 'last_state']\n",
    "    result = {}\n",
    "    for key in keys:\n",
    "        result[key] = torch.hstack([s[key] for s in sessions])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-125-9414e763290c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmerge_time_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-123-6cfbcedb6709>\u001b[0m in \u001b[0;36mmerge_time_batch\u001b[1;34m(sessions)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got list"
     ]
    }
   ],
   "source": [
    "merge_time_batch(trs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sessions_to_tensors(trajectory):\n",
    "    # trajectory['targets'] = compute_estimated_cumulative_rewards(trajectory['rewards'], trajectory['last_state']['q_value'])\n",
    "    for k, v in trajectory.items():\n",
    "        pass\n",
    "\n",
    "def compute_losses():\n",
    "    '''\n",
    "    input:\n",
    "    -\n",
    "    output: \n",
    "    -policy_loss \n",
    "    -value_loss (V = G) \n",
    "    -entropy_loss\n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A:\n",
    "    def __init__(self, x) -> None:\n",
    "        self.x = x\n",
    "\n",
    "    def incr(self):\n",
    "        self.x += 1\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return str(self.x)\n",
    "\n",
    "def create(smth, arg):\n",
    "    return smth(arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0\n"
     ]
    }
   ],
   "source": [
    "a = create(A, 0)\n",
    "a.incr()\n",
    "b = create(A, 0)\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # used for arrays\n",
    "\n",
    "import time # to get the time\n",
    "import math # needed for calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-01 16:28:52,549] Making new env: CartPole-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Danill\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gym\\envs\\registration.py:17: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "print(env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02520949, -0.0479926 ,  0.02926615, -0.01972681])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 8\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "for i in range(100):\n",
    "    state, reward, done, info = env.step(1)\n",
    "    if done:\n",
    "        print('done', i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.f = nn.Sequential(\n",
    "            nn.Linear(4, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.f(x)\n",
    "\n",
    "class ActorCartPole:\n",
    "    def __init__(self, net) -> None:\n",
    "        self.net = net\n",
    "\n",
    "    def __call__(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float)\n",
    "        action_probs = nn.functional.softmax(self.net(state)).detach().numpy()\n",
    "        action = np.random.choice([0, 1], size=None, p=action_probs)\n",
    "        return action\n",
    "\n",
    "class EnvCartPole:\n",
    "    def __init__(self, actor) -> None:\n",
    "        self.env = gym.make(\"CartPole-v1\")\n",
    "        self.actor = actor\n",
    "        self.state = self.env.reset()\n",
    "        self.reward = 0\n",
    "\n",
    "    def Next(self):\n",
    "        action = self.actor(self.state)\n",
    "        self.state, self.reward, done, info = self.env.step(action)\n",
    "        if done:\n",
    "            self.reward -= 10\n",
    "            self.state = self.env.reset()\n",
    "\n",
    "    def GetState(self):\n",
    "        return self.state\n",
    "    \n",
    "    def GetReward(self):\n",
    "        return self.reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reinforcement.cartpole import EnvCartPole, MLP, ActorCartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = MLP()\n",
    "# actor = ActorCartPole(net)\n",
    "# env = EnvCartPole(actor)\n",
    "\n",
    "# for i in range(100):\n",
    "#     print(i)\n",
    "#     # print(env.GetState())\n",
    "#     env.Next()\n",
    "#     print(env.GetReward())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "from itertools import accumulate\n",
    "import numpy as np\n",
    "\n",
    "gamma = 0.99\n",
    "rewards = np.random.randn(10)\n",
    "session = [{'reward': r} for r in rewards]\n",
    "# rewards = [s['reward'] for s in session]\n",
    "cumulative_rewards = list(accumulate(rewards[::-1], lambda x,y: x*gamma + y))[::-1]\n",
    "v_last = 0\n",
    "for i in range(len(rewards))[::-1]:\n",
    "    v_last = v_last * gamma + session[i]['reward']\n",
    "    session[i]['cum_reward'] = v_last\n",
    "\n",
    "for a, b in zip(cumulative_rewards, session):\n",
    "    print(a - b['cum_reward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "b = torch.tensor([1, 10, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  32,   16,    8,    4,    2],\n",
       "        [ 320,  160,   80,   40,   20],\n",
       "        [3200, 1600,  800,  400,  200]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Logger:\n",
    "    def __init__(self) -> None:\n",
    "        self.logs = defaultdict(list)\n",
    "\n",
    "    def log(self, key, value):\n",
    "        self.logs[key].append(value)\n",
    "\n",
    "    def plot(self):\n",
    "        ncols = 3\n",
    "        nrows = (len(self.logs) - 1) // ncols + 1\n",
    "        figure, axis = plt.subplots(3, 3, figsize=(20, 10))\n",
    "        if smoothed:\n",
    "            logs = self.smoothed_logs\n",
    "        else:\n",
    "            logs = self.logs\n",
    "        for i, (k, v) in enumerate(logs.items()):\n",
    "            r = i // 3\n",
    "            c = i % 3\n",
    "            axis[r, c].plot(v)\n",
    "            axis[r, c].set_title(k)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([32, 16,  8,  4,  2])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a35fb83ac8e6ec7d775c8e4493051254ee64ddb3600bc3fef060a4811657ed17"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
