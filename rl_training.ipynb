{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from utils import *\n",
    "from dispatch.dispatch import Dispatch, NeuralDispatch\n",
    "from simulator.base_simulator import BaseSimulator, ManualSimulator\n",
    "from simulator.simulator import Simulator\n",
    "from simulator.graphics import plot_CR, plot_counts\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from networks.encoders.point_encoder import PointEncoder\n",
    "# from networks.scoring_v1 import ScoringNet, ScoringInterface\n",
    "\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "\n",
    "import networks\n",
    "from networks.scoring_networks.net1 import ScoringNet\n",
    "from networks.encoders.gamble_encoder import GambleTripleEncoder\n",
    "# import importlib\n",
    "# importlib.reload(networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlimon8884\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "\n",
    "# num_cells = 256  # number of cells in each layer\n",
    "# lr = 3e-4\n",
    "# max_grad_norm = 1.0\n",
    "\n",
    "# frame_skip = 1\n",
    "# frames_per_batch = 1000 // frame_skip\n",
    "# # For a complete training, bring the number of frames up to 1M\n",
    "# total_frames = 200_000 // frame_skip\n",
    "\n",
    "# sub_batch_size = 4  # cardinality of the sub-samples gathered from the current data in the inner loop\n",
    "# num_epochs = 10  # optimisation steps per batch of data collected\n",
    "# clip_epsilon = (\n",
    "#     0.2  # clip value for PPO loss: see the equation in the intro for more context.\n",
    "# )\n",
    "# gamma = 0.99\n",
    "# lmbda = 0.95\n",
    "# entropy_eps = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net weights loaded successfuly!\n",
      "gamble encoder weights loaded successfuly!\n"
     ]
    }
   ],
   "source": [
    "with open('configs/network_hyperparams.json') as f:\n",
    "    hyperparams = json.load(f)    \n",
    "\n",
    "net = ScoringNet(\n",
    "    n_layers=hyperparams['n_layers'],\n",
    "    d_model=hyperparams['d_model'],\n",
    "    n_head=hyperparams['n_head'],\n",
    "    dim_ff=hyperparams['dim_ff'],\n",
    "    device=device,\n",
    "    path_weights='pretrained_models/eta_scoring_1/eta_scoring_1.pt'\n",
    ")\n",
    "encoder = GambleTripleEncoder(\n",
    "    number_enc_dim=hyperparams['number_enc_dim'], \n",
    "    d_model=hyperparams['d_model'], \n",
    "    point_enc_dim=hyperparams['point_enc_dim'],\n",
    "    path_weights='pretrained_models/assignment_cloning_model_v2/encoders/',\n",
    "    device=device\n",
    ")\n",
    "\n",
    "bounds = (Point(0, 0), Point(10, 10))\n",
    "# model = ScoringInterface(net)\n",
    "# model.load_weights('pretrained_models/assignment_cloning_model_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks.utils import get_target_assignments, get_batch_embeddings_tensors, get_batch_masks, cross_entropy_assignment_loss, get_cross_mask\n",
    "\n",
    "with open('configs/training_settings.json') as f:\n",
    "    training_settings = json.load(f)   \n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=training_settings['lr'], momentum=training_settings['momentum'])\n",
    "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-3, epochs=n_epochs, steps_per_epoch=n_iters)\n",
    "scheduler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:h747zytv) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52c5dde3148442ee90400482ab198b5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.005 MB of 0.005 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fix_ce_loss</strong> at: <a href='https://wandb.ai/limon8884/delivery-RL/runs/h747zytv' target=\"_blank\">https://wandb.ai/limon8884/delivery-RL/runs/h747zytv</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230705_154738-h747zytv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:h747zytv). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc425ee5d75244058e0a47c9a91fd952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01668758396699559, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/dbok/PycharmProjects/delivery-RL/delivery-RL/wandb/run-20230705_154753-afv392v6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/limon8884/delivery-RL/runs/afv392v6' target=\"_blank\">increase_max_items</a></strong> to <a href='https://wandb.ai/limon8884/delivery-RL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/limon8884/delivery-RL' target=\"_blank\">https://wandb.ai/limon8884/delivery-RL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/limon8884/delivery-RL/runs/afv392v6' target=\"_blank\">https://wandb.ai/limon8884/delivery-RL/runs/afv392v6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/limon8884/delivery-RL/runs/afv392v6?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1596475b0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"delivery-RL\", \n",
    "    name=f\"increase_max_items\", \n",
    "    config={\n",
    "        'hyperparams': hyperparams,\n",
    "        'training_settings': training_settings,\n",
    "        'device': device,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/transformers/attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1270718232044199"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eval func\n",
    "def get_batch_quality_metrics(dispatch: NeuralDispatch, batch_size: int, num_steps: int):\n",
    "    simulators = [Simulator() for i in range(batch_size)]\n",
    "    all_metrics = [[] for _ in range(batch_size)]\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        triples = [sim.GetState() for sim in simulators]\n",
    "        assignments = dispatch(triples)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            simulators[i].Next(assignments[i])\n",
    "            all_metrics[i].append(simulators[i].GetMetrics())\n",
    "    \n",
    "    return all_metrics\n",
    "\n",
    "def get_CR(batch_metrics):\n",
    "    return sum([metric[-1]['completed_orders'] for metric in batch_metrics]) / sum([metric[-1]['finished_orders'] for metric in batch_metrics])\n",
    "\n",
    "dsp = NeuralDispatch(net, encoder)\n",
    "get_CR(get_batch_quality_metrics(dsp, 2, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import List, Tuple, Dict\n",
    "import typing\n",
    "def update_assignment_accuracy_statistics(tgt: List[int], pred: torch.Tensor, statistics: typing.Counter):\n",
    "    '''\n",
    "    Takes a gamble correct assignments and predictions and returns the quality statistics\n",
    "    Input:\n",
    "    * tgt - list of order assignments of length ord\n",
    "    * pred - tensor of shape [ord+1, crr+2]\n",
    "    '''\n",
    "    preds_np = pred[1:, 1:].argmax(dim=-1).numpy() # [ord]\n",
    "    tgts_np = np.array(tgt)\n",
    "    n_couriers = pred.shape[-1] - 2\n",
    "    n_real_couriers = (tgts_np != -1).sum() - 1\n",
    "\n",
    "    statistics.update({\n",
    "        'fake_mistake_not_assigns': ((tgts_np == n_couriers) & (preds_np != n_couriers)).sum(),\n",
    "        'fake_mistake_assigns': ((tgts_np != n_couriers) & (tgts_np != -1) & (preds_np == n_couriers)).sum(),\n",
    "        'masked_assigns': ((tgts_np == -1) & ((preds_np < n_real_couriers) | (preds_np == n_couriers))).sum(),\n",
    "        'real_mistakes': ((tgts_np != preds_np) & (tgts_np != n_couriers) & (preds_np != n_couriers) & (tgts_np != -1)).sum(),\n",
    "        'not_masked_couriers': (tgts_np != -1).sum(),\n",
    "        'correct': ((tgts_np == preds_np) & (tgts_np != -1)).sum(),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:08<01:19,  8.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11049723756906077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:18<01:13,  9.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1132596685082873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:24<00:56,  8.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1132596685082873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:33<00:50,  8.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11878453038674033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:40<00:39,  7.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12154696132596685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:46<00:28,  7.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12430939226519337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:52<00:20,  6.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1270718232044199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [01:01<00:15,  7.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1298342541436464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [01:09<00:07,  7.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12430939226519337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:17<00:00,  7.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13259668508287292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ad7442de9cc43289839e4ed1a74d039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>cr</td><td>▁▂▂▄▅▅▆▇▅█</td></tr><tr><td>loss</td><td>▂▂▅▄▁▅▃▇█▃▆▇▇▄▃▇▃▃▂▅▆▆▄▅▄▆▃▂▃▃▅▄▅▇▆▃▇▆▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>cr</td><td>0.1326</td></tr><tr><td>loss</td><td>0.81666</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">increase_max_items</strong> at: <a href='https://wandb.ai/limon8884/delivery-RL/runs/afv392v6' target=\"_blank\">https://wandb.ai/limon8884/delivery-RL/runs/afv392v6</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230705_154753-afv392v6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "c = Counter()\n",
    "num_epochs = training_settings['num_epochs']\n",
    "num_iters = training_settings['num_iters_in_epoch']\n",
    "batch_size = training_settings['batch_size']\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    net.train()\n",
    "    encoder.train()\n",
    "    for iter in range(num_iters):\n",
    "        triples = [random_triple(bounds, max_items=20) for _ in range(batch_size)]\n",
    "        max_num_ords = max([len(triple.orders) for triple in triples])\n",
    "        max_num_crrs = max([len(triple.couriers) for triple in triples])\n",
    "\n",
    "        target_assignment_idxs = []\n",
    "        embeds = []\n",
    "        for triple in triples:\n",
    "            target_assignment_idxs.append(get_target_assignments(triple, max_num_ords, max_num_crrs))\n",
    "            embeds.append(encoder(triple, 0)[0])\n",
    "\n",
    "        batch_embs = get_batch_embeddings_tensors(embeds)\n",
    "        batch_masks = get_batch_masks(triples, device=device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred_scores, _ = net(batch_embs, batch_masks)\n",
    "        # print('gamble info:\\n', triples)\n",
    "        # print('pred_scores\\n', pred_scores)\n",
    "        # print(pred_scores.shape)\n",
    "        # print('tgt_assignments\\n', target_assignment_idxs)\n",
    "        # print('masks\\n', get_cross_mask(batch_masks))\n",
    "        # print(50 * '-')\n",
    "        loss = cross_entropy_assignment_loss(pred_scores, target_assignment_idxs, get_cross_mask(batch_masks))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        for batch_idx in range(batch_size):\n",
    "            update_assignment_accuracy_statistics(target_assignment_idxs[batch_idx], pred_scores[batch_idx], c)\n",
    "        \n",
    "        wandb.log({\"loss\": loss.item()})\n",
    "\n",
    "    dsp = NeuralDispatch(net, encoder)\n",
    "    cr = get_CR(get_batch_quality_metrics(dsp, batch_size=training_settings['eval_batch_size'], num_steps=training_settings['eval_num_steps']))\n",
    "    print(cr)\n",
    "    wandb.log({'cr': cr})\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2604\n",
      "Counter({'not_masked_couriers': 2604, 'correct': 1834, 'masked_assigns': 1179, 'real_mistakes': 474, 'fake_mistake_assigns': 159, 'fake_mistake_not_assigns': 137})\n"
     ]
    }
   ],
   "source": [
    "print(c['fake_mistake_not_assigns'] + c['fake_mistake_assigns'] + c['real_mistakes'] + c['correct'])\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 11112.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro average CR:  0.6397871797916331\n",
      "macro average CR:  0.5770145310435931\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average free couriers:  0.71\n",
      "average free orders:  7.36\n",
      "average active routes:  7.29\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5581395348837209\n"
     ]
    }
   ],
   "source": [
    "# dsp = NeuralDispatch(net)\n",
    "dsp = Dispatch()\n",
    "sim = Simulator()\n",
    "\n",
    "all_metrics = []\n",
    "for i in tqdm(range(100)):\n",
    "    metrics = sim.GetMetrics()\n",
    "    all_metrics.append(metrics)\n",
    "    state = sim.GetState()\n",
    "    sim.Next(dsp(state))\n",
    "\n",
    "plot_CR(all_metrics)\n",
    "plot_counts(all_metrics)\n",
    "print(get_CR([all_metrics]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dispatch.dispatch import BaseDispatch\n",
    "from objects.gamble_triple import GambleTriple\n",
    "from typing import List, Tuple\n",
    "# from networks.utils import (\n",
    "#     get_target_assignments, \n",
    "#     get_batch_embeddings_tensors, \n",
    "#     get_batch_masks, \n",
    "#     cross_entropy_assignment_loss,\n",
    "#     get_cross_mask, \n",
    "#     get_assignments_by_scores\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_quality_metrics(dispatch: BaseDispatch, batch_size: int, num_steps: int):\n",
    "    simulators = [Simulator() for i in range(batch_size)]\n",
    "    all_metrics = [[] for _ in range(batch_size)]\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        triples = [sim.GetState() for sim in simulators]\n",
    "        assignments = dispatch(triples)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            simulators[i].Next(assignments[i])\n",
    "            all_metrics[i].append(simulators[i].GetMetrics())\n",
    "    \n",
    "    return all_metrics\n",
    "\n",
    "def get_CR(batch_metrics):\n",
    "    return sum([metric[-1]['completed_orders'] for metric in batch_metrics]) / sum([metric[-1]['finished_orders'] for metric in batch_metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[  0.0567, -12.4470,  -1.7396,  -7.4643, -16.5806,  -3.2472,   5.3315,\n",
      "            0.3000,  -6.6798,   0.2051],\n",
      "         [  3.8313,   4.6420,   2.2801,  -3.6389,  10.6103,  -0.4599,   9.0674,\n",
      "            3.4353,  10.2023,   0.1467],\n",
      "         [  6.3932,   2.2792,   9.7091,  13.1389,  -6.4435,   8.4454,   3.6790,\n",
      "            5.2411,  -0.1722,   0.1531],\n",
      "         [  6.0260,  11.1771,   6.0526,   1.3648,   2.3024,   7.4580,   2.9952,\n",
      "           -4.0871,   7.0136,   0.1831]]])\n",
      "tensor([[[ 13.0217,  17.3822,  21.7123,  12.9640, -10.7925,  19.6936,   0.1278],\n",
      "         [  9.4766,  11.9384,   6.3004,  15.9752,  16.7912,  10.1075,   0.1464]]])\n",
      "tensor([[[  8.3659,   0.9871,   8.3749,   2.4518, -15.4744,   0.1062],\n",
      "         [  8.1867,  13.8729,  10.1306,  13.6109,  16.5886,   0.1578]]])\n",
      "tensor([[[ 7.4783, -4.8184, -7.8914, 13.1347,  0.0512],\n",
      "         [ 7.0597, 15.8740, 16.1344,  9.4404,  0.1696]]])\n",
      "tensor([[[ -2.1039,  -7.7215, -16.9115,  -0.0460],\n",
      "         [  1.7168,   8.5502,  12.4355,   0.1369]]])\n",
      "tensor([[[-1.9255,  4.2446,  0.1730],\n",
      "         [-0.4098,  6.1544,  0.1523]]])\n",
      "tensor([[[-34.6063,  -0.0505],\n",
      "         [-19.7379,   0.1379]]])\n",
      "tensor([[[-43.0852,  -0.0468],\n",
      "         [-23.1309,   0.1284],\n",
      "         [-22.2002,   0.1644]]])\n",
      "tensor([[[-4.7552e+01, -3.0840e-02],\n",
      "         [-2.4820e+01,  1.3196e-01],\n",
      "         [-2.4067e+01,  1.6619e-01],\n",
      "         [-2.5548e+01,  1.1910e-01]]])\n",
      "tensor([[[-28.8475, -30.8000,   0.0390],\n",
      "         [ -8.6253,  -7.7175,   0.1357],\n",
      "         [ -7.7913,  -6.7773,   0.1674],\n",
      "         [ -8.3026,  -2.0361,   0.1284],\n",
      "         [ -8.9981,   1.8934,   0.1408]]])\n",
      "tensor([[[-4.7230e+01,  1.3726e-02],\n",
      "         [-2.5807e+01,  1.3794e-01],\n",
      "         [-2.5070e+01,  1.7137e-01],\n",
      "         [-2.6748e+01,  1.2542e-01],\n",
      "         [-2.6163e+01,  1.4001e-01]]])\n",
      "tensor([[[-4.6825e+01,  1.3535e-02],\n",
      "         [-2.6102e+01,  1.4024e-01],\n",
      "         [-2.5286e+01,  1.7174e-01],\n",
      "         [-2.6909e+01,  1.2676e-01],\n",
      "         [-2.6342e+01,  1.4201e-01],\n",
      "         [-2.8211e+01,  1.4589e-01]]])\n",
      "tensor([[[-27.1283, -31.9305,   0.0578],\n",
      "         [-12.9459,  -2.7896,   0.1421],\n",
      "         [-12.5753,  -4.5847,   0.1719],\n",
      "         [-13.3389,  -8.0821,   0.1328],\n",
      "         [-13.3909,  -8.5094,   0.1503],\n",
      "         [-13.9123,  -6.4073,   0.1444],\n",
      "         [-12.7069,  -5.7503,   0.1714]]])\n",
      "tensor([[[-27.4319, -30.6720,   0.0834],\n",
      "         [-13.4690,  -3.2194,   0.1466],\n",
      "         [-13.0731,  -4.9031,   0.1749],\n",
      "         [-13.9452,  -8.3625,   0.1359],\n",
      "         [-13.9706,  -8.7622,   0.1532],\n",
      "         [-14.4469,  -6.7872,   0.1473],\n",
      "         [-13.2361,  -6.0530,   0.1741],\n",
      "         [-14.5369, -10.5476,   0.1340]]])\n",
      "tensor([[[-27.8457, -30.6860,   0.0810],\n",
      "         [-13.9819,  -3.7090,   0.1449],\n",
      "         [-13.6002,  -5.3580,   0.1732],\n",
      "         [-14.4259,  -8.7256,   0.1344],\n",
      "         [-14.4717,  -9.1609,   0.1517],\n",
      "         [-14.9718,  -7.2581,   0.1457],\n",
      "         [-13.7529,  -6.4775,   0.1726],\n",
      "         [-15.1034, -10.9921,   0.1329],\n",
      "         [-13.8986,  -8.1910,   0.1686]]])\n",
      "tensor([[[-28.0293, -29.3701,   0.1011],\n",
      "         [-14.2995,  -3.6833,   0.1483],\n",
      "         [-13.9213,  -5.2482,   0.1753],\n",
      "         [-14.7701,  -8.5680,   0.1370],\n",
      "         [-14.7797,  -8.9897,   0.1546],\n",
      "         [-15.3423,  -7.2116,   0.1481],\n",
      "         [-14.0756,  -6.3633,   0.1747],\n",
      "         [-15.4714, -10.8951,   0.1352],\n",
      "         [-14.2400,  -8.0658,   0.1709],\n",
      "         [-15.7710, -12.7812,   0.1540]]])\n",
      "tensor([[[-29.0122, -30.5634,   0.1024],\n",
      "         [-14.9635,  -4.4851,   0.1507],\n",
      "         [-14.5674,  -5.9756,   0.1775],\n",
      "         [-15.4341,  -9.2918,   0.1392],\n",
      "         [-15.4118,  -9.6622,   0.1569],\n",
      "         [-16.0105,  -7.9624,   0.1503],\n",
      "         [-14.7078,  -7.0446,   0.1765],\n",
      "         [-16.0625, -11.5346,   0.1371],\n",
      "         [-14.8481,  -8.7106,   0.1728],\n",
      "         [-16.3637, -13.3797,   0.1559],\n",
      "         [-15.8724,  -7.4271,   0.1565]]])\n",
      "tensor([[[-26.4486, -25.5111, -28.0533,   0.1322],\n",
      "         [-12.2060,  -4.1080,  -4.3235,   0.1908],\n",
      "         [-13.2417,  -7.3130,  -9.8553,   0.1531],\n",
      "         [-13.3247,  -7.7564,  -9.3348,   0.1677],\n",
      "         [-13.8989,  -5.9574,  -6.8759,   0.1643],\n",
      "         [-12.3320,  -5.1294,  -4.8806,   0.1893],\n",
      "         [-14.1468,  -9.6223, -10.9963,   0.1500],\n",
      "         [-12.4084,  -6.5955,  -5.6432,   0.1847],\n",
      "         [-14.0094, -11.0972,  -9.2348,   0.1678],\n",
      "         [-13.8165,  -5.7118,  -9.1741,   0.1707],\n",
      "         [-13.4638, -10.3194,  -8.7362,   0.1713]]])\n",
      "tensor([[[-26.4142, -24.8677, -27.6122,   0.1400],\n",
      "         [-13.4017,  -7.2097,  -9.8329,   0.1564],\n",
      "         [-13.4426,  -7.6060,  -9.2901,   0.1713],\n",
      "         [-14.0373,  -5.8115,  -6.8568,   0.1678],\n",
      "         [-12.4642,  -4.9923,  -4.8723,   0.1923],\n",
      "         [-14.2571,  -9.4608, -10.9287,   0.1534],\n",
      "         [-12.5286,  -6.4304,  -5.6198,   0.1882],\n",
      "         [-14.1434, -10.9213,  -9.2041,   0.1712],\n",
      "         [-13.9229,  -5.5719,  -9.1102,   0.1735],\n",
      "         [-13.5842, -10.1380,  -8.7005,   0.1748],\n",
      "         [-13.7316,  -6.8472,  -6.4177,   0.1842]]])\n",
      "tensor([[[-22.0731, -24.5361, -26.2553, -29.3467,   0.1323],\n",
      "         [-11.7871,  -7.9430,  -8.8507,  -9.6591,   0.1712],\n",
      "         [-12.5859,  -6.3967,  -6.7256, -10.8113,   0.1665],\n",
      "         [-10.8071,  -5.5411,  -4.6769,  -7.7619,   0.1908],\n",
      "         [-13.0174,  -9.6888, -10.4690,  -9.0105,   0.1528],\n",
      "         [-10.8906,  -6.7903,  -5.2585,  -6.6304,   0.1871],\n",
      "         [-12.7688, -10.9710,  -8.6132,  -6.8342,   0.1701],\n",
      "         [-12.2949,  -6.0275,  -8.7709, -12.2165,   0.1709],\n",
      "         [-12.2880, -10.3441,  -8.1916,  -7.1507,   0.1732],\n",
      "         [-12.1237,  -7.1990,  -6.0547,  -9.5565,   0.1820],\n",
      "         [-10.8813,  -4.9123,  -4.9491,  -8.2818,   0.1823]]])\n",
      "tensor([[[-18.4711, -21.6253, -23.1565, -26.3560, -19.6275, -27.7324, -20.5975,\n",
      "            0.1237],\n",
      "         [ -7.9160,  -5.6709,  -6.7389,  -7.8571,  -7.9324,  -5.0551,  -8.7699,\n",
      "            0.1699],\n",
      "         [ -8.4417,  -4.0856,  -4.5993,  -8.8743,  -1.7823, -11.9948,  -2.5140,\n",
      "            0.1662],\n",
      "         [ -7.4768,  -3.7035,  -3.1206,  -6.4051,  -4.8144,  -8.3760,  -5.1442,\n",
      "            0.1885],\n",
      "         [ -9.0011,  -7.3004,  -8.2677,  -7.2454,  -8.5641,  -5.7499,  -9.6247,\n",
      "            0.1533],\n",
      "         [ -7.3205,  -4.7624,  -3.5505,  -5.2500,  -5.5742,  -7.4625,  -5.7678,\n",
      "            0.1846],\n",
      "         [ -8.6346,  -8.5848,  -6.5972,  -5.2899,  -7.6703,  -8.6329,  -6.7855,\n",
      "            0.1672],\n",
      "         [ -8.2457,  -3.7539,  -6.5811, -10.1689,  -4.9527,  -8.2490,  -6.0337,\n",
      "            0.1708],\n",
      "         [ -8.0021,  -7.8389,  -6.0499,  -5.4502,  -7.0507,  -8.1970,  -5.7183,\n",
      "            0.1713],\n",
      "         [ -7.8517,  -4.8452,  -3.9835,  -7.6918,  -3.6527, -10.8680,  -2.4211,\n",
      "            0.1804],\n",
      "         [ -7.3921,  -2.9938,  -3.2738,  -6.7887,  -4.1749,  -7.8677,  -4.4924,\n",
      "            0.1797],\n",
      "         [ -7.2783,  -4.1343,  -5.1159,  -7.6140,  -6.0717,  -5.7674,  -6.3002,\n",
      "            0.1743]]])\n",
      "tensor([[[-17.3614, -21.2710, -23.0294, -25.9204, -19.9451, -25.9956, -20.9213,\n",
      "          -25.3095,   0.1259],\n",
      "         [ -7.9670,  -4.0367,  -4.6286,  -8.5977,  -2.1406, -11.0662,  -2.9245,\n",
      "           -9.3065,   0.1681],\n",
      "         [ -6.9505,  -3.5402,  -3.0585,  -6.0370,  -5.0491,  -7.3490,  -5.4196,\n",
      "           -5.3921,   0.1900],\n",
      "         [ -8.5313,  -7.0730,  -8.1437,  -6.8596,  -8.7150,  -4.6641,  -9.8310,\n",
      "           -4.5528,   0.1542],\n",
      "         [ -6.8125,  -4.5743,  -3.4583,  -4.8706,  -5.7688,  -6.4438,  -6.0170,\n",
      "           -4.5458,   0.1861],\n",
      "         [ -8.2991,  -8.4433,  -6.5582,  -4.9657,  -7.9054,  -7.6575,  -7.0717,\n",
      "           -6.6488,   0.1683],\n",
      "         [ -7.7351,  -3.5661,  -6.4768,  -9.7768,  -5.1421,  -7.2139,  -6.2937,\n",
      "           -7.3416,   0.1715],\n",
      "         [ -7.6090,  -7.6769,  -5.9818,  -5.0963,  -7.2642,  -7.1985,  -5.9871,\n",
      "           -6.3365,   0.1728],\n",
      "         [ -7.4245,  -4.7413,  -3.9641,  -7.3527,  -3.9288,  -9.9138,  -2.7421,\n",
      "           -8.5331,   0.1812],\n",
      "         [ -6.8675,  -2.8095,  -3.1807,  -6.3846,  -4.3786,  -6.8360,  -4.7521,\n",
      "           -5.4651,   0.1818],\n",
      "         [ -6.7066,  -3.8951,  -4.9671,  -7.1676,  -6.1964,  -4.6873,  -6.4933,\n",
      "           -4.8952,   0.1755],\n",
      "         [ -7.3188,  -5.5783,  -5.5680,  -5.0336,  -6.5964,  -5.2530,  -7.3583,\n",
      "           -4.1709,   0.1631]]])\n",
      "tensor([[[-17.4358, -21.4095, -22.8457, -26.0587, -19.7823, -26.5669, -20.5407,\n",
      "          -25.8250,   0.1286],\n",
      "         [ -7.0780,  -3.6648,  -3.1040,  -6.2676,  -5.0299,  -7.7262,  -5.3273,\n",
      "           -5.7471,   0.1913],\n",
      "         [ -8.6688,  -7.2067,  -8.1949,  -7.0891,  -8.7197,  -5.0250,  -9.7622,\n",
      "           -4.8923,   0.1554],\n",
      "         [ -6.9325,  -4.6845,  -3.4936,  -5.0893,  -5.7405,  -6.8046,  -5.9218,\n",
      "           -4.8821,   0.1870],\n",
      "         [ -8.4406,  -8.5636,  -6.6050,  -5.1977,  -7.8866,  -8.0300,  -6.9866,\n",
      "           -6.9984,   0.1695],\n",
      "         [ -7.8408,  -3.6813,  -6.5105,  -9.9841,  -5.1235,  -7.5480,  -6.2032,\n",
      "           -7.6601,   0.1730],\n",
      "         [ -7.7427,  -7.7945,  -6.0224,  -5.3169,  -7.2448,  -7.5514,  -5.8983,\n",
      "           -6.6703,   0.1739],\n",
      "         [ -7.5354,  -4.8453,  -3.9943,  -7.5638,  -3.8820, -10.2690,  -2.6295,\n",
      "           -8.8675,   0.1825],\n",
      "         [ -6.9847,  -2.9190,  -3.2131,  -6.5933,  -4.3515,  -7.1811,  -4.6543,\n",
      "           -5.7883,   0.1829],\n",
      "         [ -6.8189,  -4.0014,  -4.9991,  -7.3650,  -6.1782,  -4.9965,  -6.4090,\n",
      "           -5.1929,   0.1767],\n",
      "         [ -7.4476,  -5.6857,  -5.6061,  -5.2418,  -6.5758,  -5.5839,  -7.2716,\n",
      "           -4.4813,   0.1639],\n",
      "         [ -7.7969,  -3.5797,  -4.9958,  -7.8025,  -2.9735,  -8.5561,  -4.1960,\n",
      "           -7.2319,   0.1765]]])\n",
      "tensor([[[-17.5781, -21.4494, -21.9909, -25.2354, -19.9995, -26.6148, -20.5308,\n",
      "          -25.3124,   0.1342],\n",
      "         [ -8.8624,  -7.3409,  -8.0224,  -6.9836,  -8.8958,  -5.2826,  -9.8673,\n",
      "           -4.9105,   0.1581],\n",
      "         [ -7.1121,  -4.7968,  -3.3395,  -4.9896,  -5.8955,  -7.0329,  -6.0234,\n",
      "           -4.8896,   0.1897],\n",
      "         [ -8.6250,  -8.6535,  -6.4306,  -5.1055,  -8.0385,  -8.2413,  -7.0982,\n",
      "           -6.9867,   0.1721],\n",
      "         [ -8.0424,  -3.8578,  -6.3763,  -9.8873,  -5.3351,  -7.8043,  -6.3430,\n",
      "           -7.6823,   0.1758],\n",
      "         [ -7.9213,  -7.8853,  -5.8534,  -5.2164,  -7.3989,  -7.7596,  -6.0190,\n",
      "           -6.6523,   0.1769],\n",
      "         [ -7.7172,  -4.9724,  -3.8413,  -7.4546,  -4.0607, -10.4779,  -2.7642,\n",
      "           -8.8523,   0.1853],\n",
      "         [ -7.1523,  -3.0552,  -3.0649,  -6.4747,  -4.5227,  -7.3873,  -4.7661,\n",
      "           -5.7789,   0.1858],\n",
      "         [ -7.0023,  -4.1400,  -4.8573,  -7.2615,  -6.3489,  -5.2318,  -6.5204,\n",
      "           -5.2024,   0.1799],\n",
      "         [ -7.6231,  -5.7955,  -5.4562,  -5.1570,  -6.7268,  -5.8105,  -7.3688,\n",
      "           -4.4930,   0.1666],\n",
      "         [ -7.9744,  -3.7147,  -4.8501,  -7.6994,  -3.1503,  -8.7749,  -4.3155,\n",
      "           -7.2329,   0.1793],\n",
      "         [ -7.9319,  -4.1095,  -6.0061,  -8.8583,  -5.1494,  -7.2791,  -5.8584,\n",
      "           -7.1611,   0.1733]]])\n",
      "tensor([[[-17.7662, -21.5009, -22.0942, -25.1798, -20.0608, -26.7064, -20.7037,\n",
      "          -25.2288,   0.1350],\n",
      "         [ -7.3157,  -4.9386,  -3.5469,  -5.1180,  -6.0590,  -7.1567,  -6.2531,\n",
      "           -4.9736,   0.1902],\n",
      "         [ -8.8213,  -8.7860,  -6.6255,  -5.2228,  -8.1922,  -8.3519,  -7.3227,\n",
      "           -7.0561,   0.1727],\n",
      "         [ -8.2341,  -3.9973,  -6.5646,  -9.9865,  -5.4900,  -7.9111,  -6.5593,\n",
      "           -7.7441,   0.1763],\n",
      "         [ -8.1286,  -8.0220,  -6.0524,  -5.3381,  -7.5598,  -7.8752,  -6.2540,\n",
      "           -6.7247,   0.1775],\n",
      "         [ -7.9258,  -5.1225,  -4.0491,  -7.5778,  -4.2310, -10.5971,  -3.0044,\n",
      "           -8.9311,   0.1859],\n",
      "         [ -7.3522,  -3.1976,  -3.2626,  -6.5861,  -4.6900,  -7.4889,  -4.9942,\n",
      "           -5.8464,   0.1865],\n",
      "         [ -7.1936,  -4.2762,  -5.0448,  -7.3612,  -6.5046,  -5.3313,  -6.7362,\n",
      "           -5.2643,   0.1807],\n",
      "         [ -7.8174,  -5.9183,  -5.6381,  -5.2617,  -6.8738,  -5.9092,  -7.5780,\n",
      "           -4.5548,   0.1672],\n",
      "         [ -8.1696,  -3.8488,  -5.0359,  -7.8035,  -3.3109,  -8.8716,  -4.5352,\n",
      "           -7.2943,   0.1800],\n",
      "         [ -8.1119,  -4.2345,  -6.1815,  -8.9477,  -5.2975,  -7.3673,  -6.0672,\n",
      "           -7.2092,   0.1739],\n",
      "         [ -8.2289,  -6.5058,  -7.0274,  -6.7560,  -8.0037,  -5.0100,  -8.4801,\n",
      "           -4.9906,   0.1709]]])\n",
      "tensor([[[-17.7845, -21.2642, -21.2562, -24.7510, -19.7521, -27.1198, -20.2447,\n",
      "          -25.2244,   0.1457],\n",
      "         [ -8.9289,  -8.8128,  -6.4062,  -5.1945,  -8.2025,  -8.6515,  -7.2999,\n",
      "           -7.1741,   0.1753],\n",
      "         [ -8.3758,  -4.1068,  -6.4124,  -9.9903,  -5.5593,  -8.2597,  -6.5784,\n",
      "           -7.9113,   0.1795],\n",
      "         [ -8.2330,  -8.0561,  -5.8445,  -5.3069,  -7.5748,  -8.1748,  -6.2431,\n",
      "           -6.8418,   0.1805],\n",
      "         [ -8.0302,  -5.1786,  -3.8608,  -7.5482,  -4.2614, -10.8894,  -3.0094,\n",
      "           -9.0480,   0.1884],\n",
      "         [ -7.4562,  -3.2634,  -3.0767,  -6.5465,  -4.7214,  -7.7779,  -4.9778,\n",
      "           -5.9667,   0.1893],\n",
      "         [ -7.3030,  -4.3392,  -4.8625,  -7.3265,  -6.5304,  -5.6341,  -6.7183,\n",
      "           -5.3898,   0.1835],\n",
      "         [ -7.9226,  -5.9663,  -5.4416,  -5.2286,  -6.8980,  -6.2067,  -7.5573,\n",
      "           -4.6764,   0.1696],\n",
      "         [ -8.2782,  -3.9202,  -4.8636,  -7.7780,  -3.3597,  -9.1634,  -4.5402,\n",
      "           -7.4176,   0.1825],\n",
      "         [ -8.2336,  -4.3121,  -6.0170,  -8.9308,  -5.3498,  -7.6688,  -6.0741,\n",
      "           -7.3417,   0.1766],\n",
      "         [ -8.3387,  -6.5580,  -6.8436,  -6.7299,  -8.0345,  -5.3065,  -8.4684,\n",
      "           -5.1118,   0.1734],\n",
      "         [ -8.4751,  -6.1132,  -6.3979,  -6.4356,  -7.6054,  -5.7322,  -8.0591,\n",
      "           -5.0790,   0.1690]]])\n",
      "tensor([[[-17.9561, -21.7896, -21.1820, -24.2636, -19.5883, -27.9342, -19.8710,\n",
      "          -25.6726,   0.1412],\n",
      "         [ -8.5361,  -4.4341,  -6.5370,  -9.9129,  -5.6660,  -8.6364,  -6.5884,\n",
      "           -8.1748,   0.1798],\n",
      "         [ -8.3873,  -8.3544,  -5.9397,  -5.1842,  -7.6540,  -8.5312,  -6.2199,\n",
      "           -7.0739,   0.1808],\n",
      "         [ -8.1775,  -5.4846,  -3.9652,  -7.4449,  -4.3527, -11.2533,  -2.9963,\n",
      "           -9.2960,   0.1888],\n",
      "         [ -7.6127,  -3.5744,  -3.1835,  -6.4429,  -4.8259,  -8.1358,  -4.9875,\n",
      "           -6.2036,   0.1896],\n",
      "         [ -7.4720,  -4.6647,  -4.9922,  -7.2501,  -6.6467,  -6.0002,  -6.7404,\n",
      "           -5.6459,   0.1843],\n",
      "         [ -8.0875,  -6.2660,  -5.5431,  -5.1216,  -6.9951,  -6.5602,  -7.5594,\n",
      "           -4.9065,   0.1699],\n",
      "         [ -8.4121,  -4.2108,  -4.9573,  -7.6639,  -3.4476,  -9.4892,  -4.5320,\n",
      "           -7.6324,   0.1831],\n",
      "         [ -8.3690,  -4.6003,  -6.1192,  -8.8364,  -5.4410,  -7.9958,  -6.0731,\n",
      "           -7.5631,   0.1771],\n",
      "         [ -8.4873,  -6.8480,  -6.9429,  -6.6254,  -8.1269,  -5.6329,  -8.4647,\n",
      "           -5.3297,   0.1739],\n",
      "         [ -8.6253,  -6.3993,  -6.4952,  -6.3318,  -7.6965,  -6.0576,  -8.0558,\n",
      "           -5.2935,   0.1694],\n",
      "         [ -7.4075,  -5.3502,  -5.2925,  -6.1726,  -6.8771,  -5.2971,  -7.0666,\n",
      "           -4.8514,   0.1809]]])\n",
      "tensor([[[-17.8324, -20.9290, -21.4955, -25.1989, -19.0946, -27.8168, -19.9059,\n",
      "          -25.8597,   0.1441],\n",
      "         [ -8.4283,  -8.1098,  -6.1682,  -5.7053,  -7.5182,  -8.6639,  -6.3054,\n",
      "           -7.2989,   0.1810],\n",
      "         [ -8.1620,  -5.1813,  -4.1399,  -7.9115,  -4.1607, -11.3122,  -3.0337,\n",
      "           -9.4575,   0.1887],\n",
      "         [ -7.5863,  -3.2780,  -3.3461,  -6.8906,  -4.6384,  -8.1952,  -5.0052,\n",
      "           -6.3602,   0.1892],\n",
      "         [ -7.4404,  -4.3649,  -5.1368,  -7.6711,  -6.4433,  -6.0568,  -6.7449,\n",
      "           -5.7911,   0.1839],\n",
      "         [ -8.0901,  -5.9999,  -5.7249,  -5.5814,  -6.8358,  -6.6507,  -7.6057,\n",
      "           -5.0862,   0.1699],\n",
      "         [ -8.3623,  -3.8962,  -5.0923,  -8.0695,  -3.2407,  -9.5134,  -4.5262,\n",
      "           -7.7575,   0.1827],\n",
      "         [ -8.3062,  -4.2704,  -6.2262,  -9.2022,  -5.1992,  -8.0062,  -6.0347,\n",
      "           -7.6672,   0.1768],\n",
      "         [ -8.4831,  -6.5775,  -7.1076,  -7.0546,  -7.9409,  -5.7115,  -8.4808,\n",
      "           -5.4966,   0.1735],\n",
      "         [ -8.6219,  -6.1313,  -6.6612,  -6.7582,  -7.5177,  -6.1261,  -8.0722,\n",
      "           -5.4549,   0.1690],\n",
      "         [ -7.3873,  -5.0668,  -5.4373,  -6.5805,  -6.6871,  -5.3507,  -7.0717,\n",
      "           -4.9934,   0.1803],\n",
      "         [ -7.6046,  -5.9098,  -4.1993,  -4.9397,  -5.8446,  -7.6263,  -4.8367,\n",
      "           -6.0239,   0.1947]]])\n",
      "tensor([[[-18.0336, -21.3424, -21.4671, -24.9349, -19.0003, -28.6442, -19.4977,\n",
      "          -26.2508,   0.1403],\n",
      "         [ -8.2460,  -5.3778,  -4.1627,  -7.8069,  -4.1877, -11.6189,  -2.9182,\n",
      "           -9.6260,   0.1880],\n",
      "         [ -7.6703,  -3.4690,  -3.3614,  -6.7870,  -4.6645,  -8.4899,  -4.8907,\n",
      "           -6.5224,   0.1884],\n",
      "         [ -7.5191,  -4.5454,  -5.1536,  -7.5809,  -6.4642,  -6.3476,  -6.6332,\n",
      "           -5.9504,   0.1835],\n",
      "         [ -8.1624,  -6.1724,  -5.7228,  -5.4631,  -6.8458,  -6.9322,  -7.4748,\n",
      "           -5.2344,   0.1691],\n",
      "         [ -8.4373,  -4.0774,  -5.1036,  -7.9616,  -3.2646,  -9.7919,  -4.4060,\n",
      "           -7.9052,   0.1819],\n",
      "         [ -8.3724,  -4.4284,  -6.2262,  -9.0991,  -5.2107,  -8.2740,  -5.9124,\n",
      "           -7.8051,   0.1763],\n",
      "         [ -8.5372,  -6.7236,  -7.0908,  -6.9312,  -7.9359,  -5.9673,  -8.3370,\n",
      "           -5.6219,   0.1729],\n",
      "         [ -8.6829,  -6.2812,  -6.6503,  -6.6444,  -7.5194,  -6.3833,  -7.9388,\n",
      "           -5.5865,   0.1686],\n",
      "         [ -7.4547,  -5.2320,  -5.4420,  -6.4762,  -6.7035,  -5.6102,  -6.9538,\n",
      "           -5.1332,   0.1799],\n",
      "         [ -7.6739,  -6.0719,  -4.1938,  -4.8155,  -5.8531,  -7.8850,  -4.7089,\n",
      "           -6.1590,   0.1941],\n",
      "         [ -7.5571,  -6.0425,  -5.9184,  -6.5837,  -6.9856,  -5.8090,  -7.2987,\n",
      "           -5.7911,   0.1839]]])\n",
      "tensor([[[-17.4133, -21.0574, -20.9620, -24.7104, -18.1296, -28.9269, -18.4816,\n",
      "          -26.3395, -24.7715,   0.1391],\n",
      "         [ -7.2450,  -3.2493,  -3.1136,  -6.6058,  -4.4369,  -8.2733,  -4.5912,\n",
      "           -6.3093,  -5.6095,   0.1884],\n",
      "         [ -7.0808,  -4.3151,  -4.9174,  -7.4277,  -6.2472,  -6.1092,  -6.3702,\n",
      "           -5.7180,  -4.5509,   0.1826],\n",
      "         [ -7.8574,  -6.0091,  -5.5427,  -5.3603,  -6.6921,  -6.7727,  -7.2701,\n",
      "           -5.0683,  -5.3568,   0.1682],\n",
      "         [ -8.0467,  -3.8722,  -4.8717,  -7.7967,  -3.0658,  -9.5666,  -4.1451,\n",
      "           -7.6920,  -7.2808,   0.1819],\n",
      "         [ -7.9951,  -4.2179,  -6.0086,  -8.9553,  -5.0084,  -8.0324,  -5.6560,\n",
      "           -7.5844,  -6.5940,   0.1758],\n",
      "         [ -8.1843,  -6.5131,  -6.8822,  -6.7928,  -7.7349,  -5.7052,  -8.0991,\n",
      "           -5.3838,  -5.1684,   0.1720],\n",
      "         [ -8.3489,  -6.0804,  -6.4443,  -6.5083,  -7.3278,  -6.1396,  -7.7001,\n",
      "           -5.3613,  -5.3138,   0.1672],\n",
      "         [ -7.0262,  -4.9840,  -5.2061,  -6.3041,  -6.5036,  -5.3142,  -6.7252,\n",
      "           -4.8516,  -4.1713,   0.1787],\n",
      "         [ -7.3196,  -5.8451,  -3.9555,  -4.6549,  -5.6545,  -7.6616,  -4.4495,\n",
      "           -5.9384,  -5.8382,   0.1931],\n",
      "         [ -7.1989,  -5.8538,  -5.7234,  -6.4388,  -6.8215,  -5.5508,  -7.1064,\n",
      "           -5.5472,  -5.0711,   0.1827],\n",
      "         [ -6.9671,  -4.5794,  -4.6924,  -6.4399,  -6.1480,  -5.5646,  -6.1110,\n",
      "           -5.2210,  -4.3890,   0.1861]]])\n",
      "tensor([[[-17.3912, -20.6918, -20.6839, -24.4729, -18.3652, -28.3178, -18.9847,\n",
      "          -25.6335, -24.0340,   0.1426],\n",
      "         [ -7.0977,  -4.2395,  -4.8699,  -7.4492,  -6.3076,  -6.0947,  -6.5434,\n",
      "           -5.6102,  -4.4342,   0.1845],\n",
      "         [ -7.8743,  -5.9230,  -5.5010,  -5.4099,  -6.7575,  -6.7559,  -7.4520,\n",
      "           -4.9695,  -5.2367,   0.1703],\n",
      "         [ -8.0571,  -3.7875,  -4.8212,  -7.8184,  -3.1493,  -9.5148,  -4.3452,\n",
      "           -7.5591,  -7.1287,   0.1836],\n",
      "         [ -8.0070,  -4.1339,  -5.9506,  -8.9644,  -5.0708,  -7.9982,  -5.8295,\n",
      "           -7.4597,  -6.4565,   0.1774],\n",
      "         [ -8.1927,  -6.4133,  -6.8198,  -6.8129,  -7.7764,  -5.6862,  -8.2542,\n",
      "           -5.2740,  -5.0425,   0.1738],\n",
      "         [ -8.3543,  -5.9815,  -6.3826,  -6.5267,  -7.3687,  -6.1115,  -7.8529,\n",
      "           -5.2459,  -5.1814,   0.1690],\n",
      "         [ -7.0333,  -4.8919,  -5.1477,  -6.3224,  -6.5445,  -5.3013,  -6.8751,\n",
      "           -4.7470,  -4.0565,   0.1805],\n",
      "         [ -7.3482,  -5.7510,  -3.9210,  -4.7069,  -5.7199,  -7.6346,  -4.6451,\n",
      "           -5.8312,  -5.7145,   0.1946],\n",
      "         [ -7.2055,  -5.7542,  -5.6636,  -6.4585,  -6.8585,  -5.5378,  -7.2499,\n",
      "           -5.4426,  -4.9553,   0.1844],\n",
      "         [ -6.9814,  -4.4858,  -4.6403,  -6.4607,  -6.1926,  -5.5477,  -6.2670,\n",
      "           -5.1146,  -4.2723,   0.1878],\n",
      "         [ -7.7990,  -4.5316,  -3.9344,  -6.8686,  -3.9442,  -9.4785,  -3.0763,\n",
      "           -7.8266,  -7.3083,   0.1789]]])\n",
      "tensor([[[-17.1887, -20.5411, -20.8466, -24.9093, -18.4834, -28.0077, -19.0196,\n",
      "          -25.7650, -23.9127,   0.1435],\n",
      "         [ -7.8992,  -5.9740,  -5.6658,  -5.7215,  -6.8888,  -6.7893,  -7.5625,\n",
      "           -5.1580,  -5.3212,   0.1701],\n",
      "         [ -8.0742,  -3.8244,  -4.9698,  -8.1109,  -3.2752,  -9.5263,  -4.4474,\n",
      "           -7.7262,  -7.1925,   0.1832],\n",
      "         [ -8.0001,  -4.1471,  -6.0756,  -9.2275,  -5.1666,  -7.9913,  -5.9096,\n",
      "           -7.6021,  -6.4995,   0.1771],\n",
      "         [ -8.2044,  -6.4436,  -6.9638,  -7.1014,  -7.8824,  -5.7060,  -8.3449,\n",
      "           -5.4432,  -5.1112,   0.1735],\n",
      "         [ -8.3625,  -6.0095,  -6.5263,  -6.8127,  -7.4749,  -6.1270,  -7.9432,\n",
      "           -5.4105,  -5.2454,   0.1687],\n",
      "         [ -7.0382,  -4.9144,  -5.2828,  -6.6009,  -6.6401,  -5.3085,  -6.9538,\n",
      "           -4.9047,  -4.1148,   0.1800],\n",
      "         [ -7.3822,  -5.8016,  -4.0815,  -5.0171,  -5.8463,  -7.6767,  -4.7512,\n",
      "           -6.0225,  -5.8027,   0.1944],\n",
      "         [ -7.2154,  -5.7759,  -5.7933,  -6.7301,  -6.9536,  -5.5491,  -7.3288,\n",
      "           -5.5957,  -5.0127,   0.1840],\n",
      "         [ -6.9799,  -4.4990,  -4.7609,  -6.7199,  -6.2779,  -5.5518,  -6.3355,\n",
      "           -5.2600,  -4.3232,   0.1873],\n",
      "         [ -7.8109,  -4.5599,  -4.0727,  -7.1418,  -4.0521,  -9.4887,  -3.1684,\n",
      "           -7.9817,  -7.3661,   0.1784],\n",
      "         [ -7.0694,  -4.3593,  -3.8838,  -5.1638,  -5.5236,  -6.2853,  -5.7646,\n",
      "           -4.6662,  -4.3658,   0.1785]]])\n",
      "tensor([[[-17.5205, -21.2805, -21.5188, -24.7767, -19.2281, -27.8338, -19.8189,\n",
      "          -25.6139, -23.9946,   0.1363],\n",
      "         [ -8.2537,  -4.1525,  -5.2885,  -8.1234,  -3.6046,  -9.5218,  -4.8136,\n",
      "           -7.7210,  -7.2795,   0.1822],\n",
      "         [ -8.1665,  -4.4604,  -6.3805,  -9.2354,  -5.4812,  -7.9857,  -6.2589,\n",
      "           -7.5917,  -6.5780,   0.1762],\n",
      "         [ -8.3399,  -6.7276,  -7.2318,  -7.0644,  -8.1622,  -5.6638,  -8.6576,\n",
      "           -5.3977,  -5.1582,   0.1729],\n",
      "         [ -8.5032,  -6.2912,  -6.7969,  -6.7842,  -7.7495,  -6.0954,  -8.2534,\n",
      "           -5.3741,  -5.3001,   0.1681],\n",
      "         [ -7.1838,  -5.2039,  -5.5622,  -6.5797,  -6.9282,  -5.2771,  -7.2792,\n",
      "           -4.8708,  -4.1719,   0.1797],\n",
      "         [ -7.5467,  -6.1114,  -4.3790,  -5.0105,  -6.1506,  -7.6495,  -5.0855,\n",
      "           -5.9981,  -5.8715,   0.1936],\n",
      "         [ -7.3520,  -6.0606,  -6.0645,  -6.7083,  -7.2339,  -5.5162,  -7.6402,\n",
      "           -5.5609,  -5.0654,   0.1835],\n",
      "         [ -7.1308,  -4.7916,  -5.0426,  -6.7058,  -6.5670,  -5.5272,  -6.6602,\n",
      "           -5.2320,  -4.3849,   0.1870],\n",
      "         [ -7.9860,  -4.8703,  -4.3744,  -7.1482,  -4.3629,  -9.4763,  -3.5110,\n",
      "           -7.9675,  -7.4419,   0.1775],\n",
      "         [ -7.2193,  -4.6415,  -4.1580,  -5.1503,  -5.8042,  -6.2528,  -6.0800,\n",
      "           -4.6346,  -4.4222,   0.1779],\n",
      "         [ -7.1541,  -3.3813,  -3.3181,  -6.0478,  -4.6614,  -7.2315,  -4.7736,\n",
      "           -5.5932,  -5.0191,   0.1804]]])\n",
      "tensor([[[-17.6512, -21.0715, -21.8309, -24.9976, -18.9391, -27.7452, -19.9234,\n",
      "          -25.5251, -23.9530,   0.1355],\n",
      "         [ -8.2487,  -4.3847,  -6.5181,  -9.3854,  -5.3426,  -8.0302,  -6.3115,\n",
      "           -7.6182,  -6.6109,   0.1758],\n",
      "         [ -8.4239,  -6.6553,  -7.3690,  -7.2176,  -8.0272,  -5.7134,  -8.7081,\n",
      "           -5.4320,  -5.1958,   0.1722],\n",
      "         [ -8.5854,  -6.2223,  -6.9393,  -6.9420,  -7.6166,  -6.1453,  -8.3034,\n",
      "           -5.4129,  -5.3413,   0.1674],\n",
      "         [ -7.2612,  -5.1327,  -5.6978,  -6.7301,  -6.8000,  -5.3205,  -7.3326,\n",
      "           -4.9014,  -4.2068,   0.1791],\n",
      "         [ -7.6534,  -6.0558,  -4.5423,  -5.1954,  -6.0397,  -7.7123,  -5.1598,\n",
      "           -6.0509,  -5.9274,   0.1927],\n",
      "         [ -7.4395,  -5.9921,  -6.2003,  -6.8594,  -7.1077,  -5.5580,  -7.6938,\n",
      "           -5.5913,  -5.0996,   0.1830],\n",
      "         [ -7.2180,  -4.7242,  -5.1817,  -6.8598,  -6.4443,  -5.5719,  -6.7201,\n",
      "           -5.2637,  -4.4216,   0.1865],\n",
      "         [ -8.0782,  -4.7995,  -4.5198,  -7.3128,  -4.2359,  -9.5191,  -3.5739,\n",
      "           -8.0009,  -7.4788,   0.1770],\n",
      "         [ -7.2961,  -4.5704,  -4.2931,  -5.3066,  -5.6777,  -6.2935,  -6.1283,\n",
      "           -4.6701,  -4.4599,   0.1773],\n",
      "         [ -7.2401,  -3.3115,  -3.4545,  -6.1992,  -4.5357,  -7.2652,  -4.8268,\n",
      "           -5.6223,  -5.0528,   0.1800],\n",
      "         [ -7.6590,  -4.0788,  -3.5992,  -6.4347,  -4.0418,  -8.7070,  -3.7057,\n",
      "           -7.0212,  -6.5652,   0.1821]]])\n",
      "tensor([[[-17.7306, -21.2107, -22.1644, -25.2894, -18.9616, -27.9243, -20.0514,\n",
      "          -25.8175, -24.2436,   0.1352],\n",
      "         [ -8.5041,  -6.7519,  -7.5558,  -7.4101,  -8.0699,  -5.8427,  -8.8125,\n",
      "           -5.6089,  -5.3613,   0.1720],\n",
      "         [ -8.6609,  -6.3126,  -7.1215,  -7.1315,  -7.6534,  -6.2711,  -8.4011,\n",
      "           -5.5858,  -5.5027,   0.1673],\n",
      "         [ -7.3402,  -5.2284,  -5.8827,  -6.9222,  -6.8451,  -5.4440,  -7.4372,\n",
      "           -5.0758,  -4.3697,   0.1791],\n",
      "         [ -7.7463,  -6.1634,  -4.7343,  -5.3961,  -6.0912,  -7.8529,  -5.2653,\n",
      "           -6.2391,  -6.1052,   0.1926],\n",
      "         [ -7.5184,  -6.0877,  -6.3810,  -7.0480,  -7.1523,  -5.6788,  -7.7965,\n",
      "           -5.7625,  -5.2590,   0.1830],\n",
      "         [ -7.2954,  -4.8143,  -5.3599,  -7.0464,  -6.4844,  -5.6902,  -6.8178,\n",
      "           -5.4331,  -4.5795,   0.1865],\n",
      "         [ -8.1624,  -4.8936,  -4.7016,  -7.5065,  -4.2708,  -9.6519,  -3.6647,\n",
      "           -8.1821,  -7.6491,   0.1769],\n",
      "         [ -7.3682,  -4.6518,  -4.4585,  -5.4813,  -5.7105,  -6.4071,  -6.2140,\n",
      "           -4.8306,  -4.6106,   0.1773],\n",
      "         [ -7.3150,  -3.3920,  -3.6230,  -6.3807,  -4.5679,  -7.3832,  -4.9151,\n",
      "           -5.7874,  -5.2073,   0.1800],\n",
      "         [ -7.7408,  -4.1665,  -3.7738,  -6.6229,  -4.0755,  -8.8330,  -3.7938,\n",
      "           -7.1959,  -6.7293,   0.1821],\n",
      "         [ -7.7104,  -3.5452,  -4.8804,  -7.6845,  -5.0389,  -7.0373,  -5.5662,\n",
      "           -6.1846,  -5.3308,   0.1785]]])\n",
      "tensor([[[-17.8801, -21.9881, -22.9048, -25.2339, -20.2188, -27.1498, -21.2121,\n",
      "          -25.3990, -23.8829,   0.1266],\n",
      "         [ -8.7255,  -6.6080,  -7.4229,  -7.1438,  -8.0855,  -6.0438,  -8.8241,\n",
      "           -5.4756,  -5.4309,   0.1665],\n",
      "         [ -7.4100,  -5.5256,  -6.1846,  -6.9383,  -7.2768,  -5.2204,  -7.8581,\n",
      "           -4.9704,  -4.3031,   0.1783],\n",
      "         [ -7.8553,  -6.5048,  -5.0724,  -5.4460,  -6.5666,  -7.6537,  -5.7237,\n",
      "           -6.1631,  -6.0686,   0.1916],\n",
      "         [ -7.5814,  -6.3817,  -6.6772,  -7.0664,  -7.5782,  -5.4613,  -8.2087,\n",
      "           -5.6588,  -5.1924,   0.1822],\n",
      "         [ -7.3772,  -5.1154,  -5.6627,  -7.0682,  -6.9179,  -5.4791,  -7.2418,\n",
      "           -5.3342,  -4.5207,   0.1857],\n",
      "         [ -8.2757,  -5.2266,  -5.0313,  -7.5542,  -4.7419,  -9.4623,  -4.1186,\n",
      "           -8.1037,  -7.6096,   0.1755],\n",
      "         [ -7.4489,  -4.9465,  -4.7571,  -5.5011,  -6.1362,  -6.1908,  -6.6283,\n",
      "           -4.7297,  -4.5489,   0.1764],\n",
      "         [ -7.4001,  -3.6899,  -3.9261,  -6.4058,  -5.0035,  -7.1756,  -5.3405,\n",
      "           -5.6908,  -5.1510,   0.1789],\n",
      "         [ -7.8453,  -4.4823,  -4.0901,  -6.6635,  -4.5298,  -8.6413,  -4.2326,\n",
      "           -7.1133,  -6.6852,   0.1806],\n",
      "         [ -7.7891,  -3.8361,  -5.1730,  -7.7013,  -5.4672,  -6.8273,  -5.9853,\n",
      "           -6.0832,  -5.2700,   0.1774],\n",
      "         [ -7.8327,  -3.8591,  -4.5487,  -6.9126,  -4.0618,  -7.9951,  -4.7102,\n",
      "           -6.5534,  -6.1798,   0.1707]]])\n",
      "tensor([[[-17.8754, -21.9841, -23.0345, -25.3279, -20.3017, -27.0228, -21.2993,\n",
      "          -25.3875, -23.8320,   0.1262],\n",
      "         [ -7.4484,  -5.5871,  -6.3134,  -7.0634,  -7.3613,  -5.2422,  -7.9463,\n",
      "           -5.0434,  -4.3573,   0.1785],\n",
      "         [ -7.9071,  -6.5777,  -5.2115,  -5.5802,  -6.6624,  -7.6851,  -5.8236,\n",
      "           -6.2441,  -6.1314,   0.1917],\n",
      "         [ -7.6162,  -6.4419,  -6.8012,  -7.1839,  -7.6608,  -5.4783,  -8.2940,\n",
      "           -5.7245,  -5.2413,   0.1824],\n",
      "         [ -7.4112,  -5.1717,  -5.7834,  -7.1825,  -6.9969,  -5.4939,  -7.3234,\n",
      "           -5.3973,  -4.5673,   0.1857],\n",
      "         [ -8.3096,  -5.2824,  -5.1519,  -7.6644,  -4.8215,  -9.4732,  -4.2013,\n",
      "           -8.1628,  -7.6525,   0.1755],\n",
      "         [ -7.4856,  -5.0029,  -4.8792,  -5.6164,  -6.2179,  -6.2052,  -6.7116,\n",
      "           -4.7929,  -4.5966,   0.1765],\n",
      "         [ -7.4272,  -3.7406,  -4.0424,  -6.5119,  -5.0796,  -7.1813,  -5.4192,\n",
      "           -5.7449,  -5.1909,   0.1790],\n",
      "         [ -7.8750,  -4.5351,  -4.2065,  -6.7709,  -4.6076,  -8.6501,  -4.3115,\n",
      "           -7.1706,  -6.7274,   0.1806],\n",
      "         [ -7.8125,  -3.8802,  -5.2823,  -7.8007,  -5.5355,  -6.8274,  -6.0566,\n",
      "           -6.1320,  -5.3036,   0.1774],\n",
      "         [ -7.8615,  -3.9062,  -4.6597,  -7.0144,  -4.1340,  -7.9988,  -4.7849,\n",
      "           -6.6044,  -6.2162,   0.1707],\n",
      "         [ -8.0819,  -5.6692,  -6.0208,  -5.9307,  -6.9247,  -5.8454,  -7.5898,\n",
      "           -4.7866,  -4.8792,   0.1735]]])\n",
      "tensor([[[-17.8056, -22.4344, -23.1295, -24.9817, -21.1175, -26.2724, -21.8815,\n",
      "          -24.9099, -23.3544,   0.1232],\n",
      "         [ -7.9244,  -6.8116,  -5.3291,  -5.5532,  -6.9991,  -7.4922,  -6.0825,\n",
      "           -6.1466,  -6.0320,   0.1916],\n",
      "         [ -7.6103,  -6.6501,  -6.8994,  -7.1450,  -7.9714,  -5.2859,  -8.5231,\n",
      "           -5.6197,  -5.1361,   0.1825],\n",
      "         [ -7.4180,  -5.3861,  -5.8868,  -7.1452,  -7.3122,  -5.3085,  -7.5624,\n",
      "           -5.2962,  -4.4683,   0.1857],\n",
      "         [ -8.3394,  -5.5246,  -5.2795,  -7.6457,  -5.1740,  -9.2975,  -4.4733,\n",
      "           -8.0753,  -7.5647,   0.1751],\n",
      "         [ -7.4992,  -5.2200,  -4.9889,  -5.5856,  -6.5342,  -6.0156,  -6.9507,\n",
      "           -4.6945,  -4.4973,   0.1766],\n",
      "         [ -7.4399,  -3.9635,  -4.1568,  -6.4823,  -5.4048,  -6.9980,  -5.6652,\n",
      "           -5.6505,  -5.0969,   0.1790],\n",
      "         [ -7.8916,  -4.7627,  -4.3216,  -6.7436,  -4.9418,  -8.4700,  -4.5656,\n",
      "           -7.0764,  -6.6325,   0.1802],\n",
      "         [ -7.8319,  -4.1051,  -5.3974,  -7.7741,  -5.8648,  -6.6534,  -6.3073,\n",
      "           -6.0426,  -5.2165,   0.1773],\n",
      "         [ -7.8891,  -4.1341,  -4.7810,  -6.9984,  -4.4687,  -7.8327,  -5.0432,\n",
      "           -6.5221,  -6.1339,   0.1705],\n",
      "         [ -8.0911,  -5.8751,  -6.1207,  -5.8983,  -7.2331,  -5.6606,  -7.8218,\n",
      "           -4.6863,  -4.7786,   0.1735],\n",
      "         [ -8.1298,  -4.1569,  -5.6425,  -7.6020,  -5.0740,  -7.1905,  -5.9242,\n",
      "           -6.2189,  -5.6742,   0.1775]]])\n",
      "tensor([[[-17.7877, -22.7769, -23.2862, -24.6223, -21.5237, -25.9225, -22.2415,\n",
      "          -24.5767, -23.1431,   0.1201],\n",
      "         [ -7.6506,  -6.8479,  -7.0153,  -7.0718,  -8.1583,  -5.2423,  -8.7018,\n",
      "           -5.5707,  -5.1360,   0.1821],\n",
      "         [ -7.4759,  -5.5952,  -6.0153,  -7.0809,  -7.5093,  -5.2741,  -7.7571,\n",
      "           -5.2551,  -4.4779,   0.1853],\n",
      "         [ -8.4118,  -5.7522,  -5.4260,  -7.5974,  -5.4011,  -9.2653,  -4.6945,\n",
      "           -8.0400,  -7.5774,   0.1743],\n",
      "         [ -7.5476,  -5.4198,  -5.1051,  -5.5098,  -6.7209,  -5.9660,  -7.1305,\n",
      "           -4.6429,  -4.4942,   0.1761],\n",
      "         [ -7.4986,  -4.1734,  -4.2826,  -6.4126,  -5.6047,  -6.9525,  -5.8595,\n",
      "           -5.6032,  -5.0988,   0.1784],\n",
      "         [ -7.9590,  -4.9804,  -4.4568,  -6.6851,  -5.1550,  -8.4299,  -4.7746,\n",
      "           -7.0349,  -6.6395,   0.1794],\n",
      "         [ -7.8928,  -4.3175,  -5.5293,  -7.7169,  -6.0710,  -6.6195,  -6.5087,\n",
      "           -6.0044,  -5.2256,   0.1768],\n",
      "         [ -7.9581,  -4.3491,  -4.9170,  -6.9479,  -4.6814,  -7.8021,  -5.2518,\n",
      "           -6.4877,  -6.1463,   0.1698],\n",
      "         [ -8.1372,  -6.0675,  -6.2314,  -5.8249,  -7.4180,  -5.6143,  -8.0016,\n",
      "           -4.6337,  -4.7738,   0.1727],\n",
      "         [ -8.1979,  -4.3684,  -5.7762,  -7.5545,  -5.2811,  -7.1656,  -6.1279,\n",
      "           -6.1886,  -5.6907,   0.1770],\n",
      "         [ -7.8126,  -5.0438,  -4.2183,  -6.2735,  -5.4392,  -7.9694,  -4.5893,\n",
      "           -6.6430,  -6.1963,   0.1807]]])\n",
      "tensor([[[-17.6637, -23.3402, -23.5825, -24.3522, -22.4525, -25.0795, -22.9607,\n",
      "          -24.1777, -22.7466,   0.1178],\n",
      "         [ -7.4831,  -5.8830,  -6.2173,  -7.0818,  -7.8885,  -5.0614,  -8.0788,\n",
      "           -5.1992,  -4.4309,   0.1850],\n",
      "         [ -8.4339,  -6.0614,  -5.6432,  -7.6122,  -5.8076,  -9.0647,  -5.0372,\n",
      "           -7.9953,  -7.5392,   0.1737],\n",
      "         [ -7.5613,  -5.7035,  -5.3036,  -5.5081,  -7.0930,  -5.7478,  -7.4420,\n",
      "           -4.5838,  -4.4428,   0.1758],\n",
      "         [ -7.5145,  -4.4635,  -4.4855,  -6.4142,  -5.9852,  -6.7434,  -6.1783,\n",
      "           -5.5508,  -5.0550,   0.1780],\n",
      "         [ -7.9754,  -5.2760,  -4.6618,  -6.6933,  -5.5430,  -8.2285,  -5.1006,\n",
      "           -6.9872,  -6.5994,   0.1787],\n",
      "         [ -7.9163,  -4.6144,  -5.7379,  -7.7281,  -6.4590,  -6.4232,  -6.8343,\n",
      "           -5.9621,  -5.1923,   0.1766],\n",
      "         [ -7.9844,  -4.6398,  -5.1247,  -6.9623,  -5.0701,  -7.6103,  -5.5844,\n",
      "           -6.4446,  -6.1104,   0.1692],\n",
      "         [ -8.1467,  -6.3429,  -6.4242,  -5.8229,  -7.7849,  -5.4014,  -8.3117,\n",
      "           -4.5731,  -4.7211,   0.1722],\n",
      "         [ -8.2212,  -4.6534,  -5.9807,  -7.5692,  -5.6620,  -6.9739,  -6.4514,\n",
      "           -6.1457,  -5.6550,   0.1766],\n",
      "         [ -7.8325,  -5.3303,  -4.4163,  -6.2787,  -5.8120,  -7.7692,  -4.9014,\n",
      "           -6.5924,  -6.1546,   0.1800],\n",
      "         [ -8.0045,  -4.4735,  -5.0917,  -6.8192,  -5.1527,  -7.2761,  -5.7354,\n",
      "           -6.0982,  -5.7938,   0.1779]]])\n",
      "tensor([[[-17.6528, -23.7091, -23.9561, -24.3554, -23.1665, -24.4588, -23.5992,\n",
      "          -23.9247, -22.4507,   0.1169],\n",
      "         [ -8.5124,  -6.3049,  -5.8755,  -7.7191,  -6.1430,  -8.9313,  -5.3633,\n",
      "           -7.9974,  -7.5220,   0.1733],\n",
      "         [ -7.6173,  -5.9235,  -5.5150,  -5.5905,  -7.3955,  -5.5895,  -7.7366,\n",
      "           -4.5683,  -4.4079,   0.1756],\n",
      "         [ -7.5718,  -4.6787,  -4.6938,  -6.4949,  -6.2848,  -6.5863,  -6.4704,\n",
      "           -5.5332,  -5.0199,   0.1778],\n",
      "         [ -8.0415,  -5.5055,  -4.8809,  -6.7877,  -5.8576,  -8.0876,  -5.4066,\n",
      "           -6.9817,  -6.5754,   0.1783],\n",
      "         [ -7.9754,  -4.8281,  -5.9447,  -7.8137,  -6.7610,  -6.2757,  -7.1312,\n",
      "           -5.9473,  -5.1599,   0.1764],\n",
      "         [ -8.0558,  -4.8598,  -5.3386,  -7.0579,  -5.3789,  -7.4750,  -5.8892,\n",
      "           -6.4390,  -6.0872,   0.1689],\n",
      "         [ -8.1904,  -6.5426,  -6.6161,  -5.8912,  -8.0681,  -5.2383,  -8.5903,\n",
      "           -4.5438,  -4.6743,   0.1719],\n",
      "         [ -8.2884,  -4.8647,  -6.1894,  -7.6626,  -5.9630,  -6.8347,  -6.7491,\n",
      "           -6.1360,  -5.6274,   0.1764],\n",
      "         [ -7.8992,  -5.5506,  -4.6266,  -6.3685,  -6.1101,  -7.6296,  -5.1932,\n",
      "           -6.5842,  -6.1295,   0.1795],\n",
      "         [ -8.0778,  -4.6900,  -5.3030,  -6.9169,  -5.4536,  -7.1456,  -6.0328,\n",
      "           -6.0949,  -5.7734,   0.1777],\n",
      "         [ -7.6773,  -4.5375,  -4.6353,  -6.4507,  -5.6638,  -6.8589,  -5.8674,\n",
      "           -5.8227,  -5.4092,   0.1804]]])\n",
      "tensor([[[-17.7099, -23.9857, -23.7771, -24.3245, -23.0828, -24.9647, -23.1728,\n",
      "          -24.3889, -22.9186,   0.1162],\n",
      "         [ -7.6813,  -6.0842,  -5.5125,  -5.6383,  -7.4503,  -5.8121,  -7.6486,\n",
      "           -4.7856,  -4.6186,   0.1756],\n",
      "         [ -7.6257,  -4.8364,  -4.6866,  -6.5357,  -6.3324,  -6.8075,  -6.3743,\n",
      "           -5.7486,  -5.2288,   0.1776],\n",
      "         [ -8.0799,  -5.6564,  -4.8644,  -6.8174,  -5.8982,  -8.2993,  -5.2980,\n",
      "           -7.1901,  -6.7763,   0.1782],\n",
      "         [ -8.0208,  -4.9814,  -5.9371,  -7.8546,  -6.8085,  -6.4875,  -7.0367,\n",
      "           -6.1561,  -5.3614,   0.1763],\n",
      "         [ -8.1008,  -5.0097,  -5.3247,  -7.0907,  -5.4204,  -7.6829,  -5.7839,\n",
      "           -6.6442,  -6.2857,   0.1687],\n",
      "         [ -8.2488,  -6.6957,  -6.6107,  -5.9337,  -8.1229,  -5.4460,  -8.5015,\n",
      "           -4.7470,  -4.8707,   0.1718],\n",
      "         [ -8.3334,  -5.0110,  -6.1768,  -7.6962,  -6.0073,  -7.0345,  -6.6492,\n",
      "           -6.3343,  -5.8194,   0.1763],\n",
      "         [ -7.9351,  -5.6907,  -4.6035,  -6.3916,  -6.1427,  -7.8305,  -5.0800,\n",
      "           -6.7810,  -6.3184,   0.1794],\n",
      "         [ -8.1193,  -4.8300,  -5.2836,  -6.9435,  -5.4889,  -7.3414,  -5.9241,\n",
      "           -6.2887,  -5.9602,   0.1776],\n",
      "         [ -7.7193,  -4.6783,  -4.6175,  -6.4789,  -5.7009,  -7.0577,  -5.7630,\n",
      "           -6.0178,  -5.5977,   0.1803],\n",
      "         [ -8.2670,  -5.1243,  -6.1953,  -7.5710,  -6.1161,  -6.7876,  -6.6241,\n",
      "           -6.2247,  -5.7063,   0.1745]]])\n",
      "tensor([[[-17.7217, -24.2706, -23.5793, -23.7355, -23.4904, -24.6092, -23.4478,\n",
      "          -23.9047, -22.5672,   0.1163],\n",
      "         [ -7.6728,  -5.0232,  -4.7010,  -6.3872,  -6.5574,  -6.7395,  -6.5515,\n",
      "           -5.6315,  -5.1593,   0.1784],\n",
      "         [ -8.1235,  -5.8375,  -4.8738,  -6.6660,  -6.1233,  -8.2260,  -5.4765,\n",
      "           -7.0658,  -6.6992,   0.1786],\n",
      "         [ -8.0658,  -5.1711,  -5.9531,  -7.7093,  -7.0360,  -6.4295,  -7.2143,\n",
      "           -6.0460,  -5.3001,   0.1772],\n",
      "         [ -8.1486,  -5.1934,  -5.3400,  -6.9498,  -5.6496,  -7.6184,  -5.9645,\n",
      "           -6.5305,  -6.2182,   0.1694],\n",
      "         [ -8.2748,  -6.8532,  -6.6007,  -5.7732,  -8.3204,  -5.3680,  -8.6506,\n",
      "           -4.6177,  -4.7870,   0.1725],\n",
      "         [ -8.3776,  -5.1921,  -6.1890,  -7.5551,  -6.2285,  -6.9787,  -6.8219,\n",
      "           -6.2267,  -5.7590,   0.1771],\n",
      "         [ -7.9770,  -5.8612,  -4.6084,  -6.2406,  -6.3534,  -7.7555,  -5.2462,\n",
      "           -6.6559,  -6.2408,   0.1801],\n",
      "         [ -8.1661,  -5.0081,  -5.2964,  -6.8036,  -5.7090,  -7.2801,  -6.0966,\n",
      "           -6.1779,  -5.8955,   0.1784],\n",
      "         [ -7.7573,  -4.8520,  -4.6259,  -6.3329,  -5.9152,  -6.9902,  -5.9305,\n",
      "           -5.9004,  -5.5265,   0.1811],\n",
      "         [ -8.3073,  -5.2982,  -6.2040,  -7.4297,  -6.3304,  -6.7289,  -6.7919,\n",
      "           -6.1147,  -5.6426,   0.1754],\n",
      "         [ -8.1717,  -4.9859,  -5.7714,  -7.1433,  -6.1731,  -6.6446,  -6.5616,\n",
      "           -5.9272,  -5.4906,   0.1750]]])\n",
      "tensor([[[-17.6860, -23.8115, -23.0860, -23.8292, -23.0053, -24.9907, -22.9455,\n",
      "          -24.1177, -22.6598,   0.1217],\n",
      "         [ -8.1397,  -5.7375,  -4.7452,  -6.7672,  -6.0156,  -8.4339,  -5.3659,\n",
      "           -7.2077,  -6.7845,   0.1800],\n",
      "         [ -8.0896,  -5.0845,  -5.8334,  -7.8140,  -6.9300,  -6.6510,  -7.1017,\n",
      "           -6.1988,  -5.3991,   0.1784],\n",
      "         [ -8.1679,  -5.1040,  -5.2164,  -7.0490,  -5.5492,  -7.8239,  -5.8558,\n",
      "           -6.6717,  -6.3050,   0.1706],\n",
      "         [ -8.3165,  -6.7732,  -6.4883,  -5.8952,  -8.2307,  -5.6016,  -8.5512,\n",
      "           -4.7823,  -4.8946,   0.1737],\n",
      "         [ -8.3958,  -5.1009,  -6.0623,  -7.6512,  -6.1226,  -7.1858,  -6.7059,\n",
      "           -6.3686,  -5.8468,   0.1783],\n",
      "         [ -7.9895,  -5.7561,  -4.4744,  -6.3354,  -6.2424,  -7.9594,  -5.1309,\n",
      "           -6.7924,  -6.3212,   0.1813],\n",
      "         [ -8.1824,  -4.9145,  -5.1695,  -6.8967,  -5.6061,  -7.4781,  -5.9838,\n",
      "           -6.3132,  -5.9762,   0.1794],\n",
      "         [ -7.7737,  -4.7596,  -4.4999,  -6.4264,  -5.8117,  -7.1902,  -5.8187,\n",
      "           -6.0359,  -5.6090,   0.1822],\n",
      "         [ -8.3292,  -5.2089,  -6.0792,  -7.5234,  -6.2286,  -6.9304,  -6.6807,\n",
      "           -6.2522,  -5.7279,   0.1764],\n",
      "         [ -8.1899,  -4.8958,  -5.6458,  -7.2340,  -6.0705,  -6.8422,  -6.4489,\n",
      "           -6.0617,  -5.5731,   0.1760],\n",
      "         [ -8.1161,  -6.4784,  -6.2780,  -5.9626,  -7.9839,  -5.3939,  -8.1565,\n",
      "           -4.8318,  -4.7663,   0.1746]]])\n",
      "tensor([[[-17.6570, -23.7118, -22.5562, -23.5022, -22.6229, -25.2989, -22.3953,\n",
      "          -24.1480, -22.7260,   0.1222],\n",
      "         [ -8.1072,  -5.1179,  -5.7037,  -7.7611,  -6.8711,  -6.8410,  -6.9585,\n",
      "           -6.2813,  -5.4897,   0.1791],\n",
      "         [ -8.1837,  -5.1320,  -5.0789,  -6.9919,  -5.4899,  -8.0049,  -5.7083,\n",
      "           -6.7485,  -6.3885,   0.1711],\n",
      "         [ -8.3532,  -6.8055,  -6.3606,  -5.8575,  -8.1786,  -5.8007,  -8.4115,\n",
      "           -4.8732,  -4.9904,   0.1744],\n",
      "         [ -8.4133,  -5.1289,  -5.9256,  -7.5940,  -6.0638,  -7.3659,  -6.5597,\n",
      "           -6.4447,  -5.9308,   0.1789],\n",
      "         [ -8.0005,  -5.7709,  -4.3332,  -6.2740,  -6.1751,  -8.1345,  -4.9815,\n",
      "           -6.8621,  -6.3966,   0.1820],\n",
      "         [ -8.1958,  -4.9363,  -5.0301,  -6.8355,  -5.5434,  -7.6502,  -5.8329,\n",
      "           -6.3834,  -6.0528,   0.1799],\n",
      "         [ -7.7855,  -4.7826,  -4.3635,  -6.3683,  -5.7491,  -7.3658,  -5.6710,\n",
      "           -6.1080,  -5.6880,   0.1828],\n",
      "         [ -8.3505,  -5.2370,  -5.9499,  -7.4722,  -6.1753,  -7.1069,  -6.5440,\n",
      "           -6.3273,  -5.8101,   0.1770],\n",
      "         [ -8.2072,  -4.9227,  -5.5137,  -7.1788,  -6.0144,  -7.0152,  -6.3071,\n",
      "           -6.1341,  -5.6529,   0.1766],\n",
      "         [ -8.1509,  -6.5098,  -6.1579,  -5.9272,  -7.9358,  -5.5823,  -8.0252,\n",
      "           -4.9174,  -4.8565,   0.1753],\n",
      "         [ -8.1588,  -5.6766,  -6.1207,  -7.5722,  -6.6185,  -6.6997,  -6.7182,\n",
      "           -6.5045,  -5.8840,   0.1748]]])\n",
      "tensor([[[-17.7543, -23.5385, -22.6526, -23.8108, -22.3848, -25.6730, -22.2927,\n",
      "          -24.4452, -23.0121,   0.1220],\n",
      "         [ -8.2325,  -5.0899,  -5.1350,  -7.1310,  -5.4395,  -8.1525,  -5.7007,\n",
      "           -6.8751,  -6.5039,   0.1709],\n",
      "         [ -8.4176,  -6.7741,  -6.4274,  -6.0133,  -8.1391,  -5.9686,  -8.4137,\n",
      "           -5.0172,  -5.1219,   0.1740],\n",
      "         [ -8.4567,  -5.0818,  -5.9751,  -7.7249,  -6.0073,  -7.5067,  -6.5457,\n",
      "           -6.5644,  -6.0390,   0.1786],\n",
      "         [ -8.0482,  -5.7247,  -4.3881,  -6.4155,  -6.1239,  -8.2816,  -4.9745,\n",
      "           -6.9887,  -6.5104,   0.1817],\n",
      "         [ -8.2386,  -4.8902,  -5.0809,  -6.9672,  -5.4901,  -7.7878,  -5.8196,\n",
      "           -6.5019,  -6.1592,   0.1795],\n",
      "         [ -7.8316,  -4.7371,  -4.4138,  -6.5012,  -5.6970,  -7.5080,  -5.6591,\n",
      "           -6.2292,  -5.7986,   0.1825],\n",
      "         [ -8.3884,  -5.1852,  -5.9924,  -7.5943,  -6.1152,  -7.2382,  -6.5253,\n",
      "           -6.4379,  -5.9097,   0.1766],\n",
      "         [ -8.2463,  -4.8725,  -5.5585,  -7.3029,  -5.9578,  -7.1457,  -6.2902,\n",
      "           -6.2455,  -5.7529,   0.1762],\n",
      "         [ -8.2037,  -6.4698,  -6.2134,  -6.0647,  -7.8899,  -5.7289,  -8.0188,\n",
      "           -5.0423,  -4.9703,   0.1749],\n",
      "         [ -8.1956,  -5.6251,  -6.1618,  -7.6881,  -6.5582,  -6.8247,  -6.6988,\n",
      "           -6.6096,  -5.9793,   0.1745],\n",
      "         [ -7.4086,  -5.2462,  -5.0602,  -6.3750,  -6.9992,  -5.5091,  -6.7916,\n",
      "           -5.1350,  -4.4199,   0.1831]]])\n",
      "tensor([[[-17.7603, -23.1442, -22.6102, -24.2513, -21.7727, -26.1365, -21.8121,\n",
      "          -24.9008, -23.3726,   0.1243],\n",
      "         [ -8.4853,  -6.6843,  -6.4682,  -6.2549,  -7.9759,  -6.1961,  -8.2961,\n",
      "           -5.2510,  -5.3048,   0.1739],\n",
      "         [ -8.4922,  -4.9688,  -5.9919,  -7.9369,  -5.8178,  -7.7048,  -6.4037,\n",
      "           -6.7699,  -6.1948,   0.1786],\n",
      "         [ -8.0884,  -5.6134,  -4.4088,  -6.6352,  -5.9396,  -8.4897,  -4.8370,\n",
      "           -7.2022,  -6.6734,   0.1818],\n",
      "         [ -8.2757,  -4.7783,  -5.0983,  -7.1784,  -5.3039,  -7.9835,  -5.6767,\n",
      "           -6.7061,  -6.3138,   0.1794],\n",
      "         [ -7.8718,  -4.6274,  -4.4310,  -6.7125,  -5.5136,  -7.7069,  -5.5189,\n",
      "           -6.4349,  -5.9569,   0.1825],\n",
      "         [ -8.4222,  -5.0724,  -6.0061,  -7.7990,  -5.9285,  -7.4264,  -6.3842,\n",
      "           -6.6350,  -6.0580,   0.1765],\n",
      "         [ -8.2858,  -4.7652,  -5.5773,  -7.5114,  -5.7779,  -7.3367,  -6.1535,\n",
      "           -6.4460,  -5.9047,   0.1761],\n",
      "         [ -8.2630,  -6.3777,  -6.2485,  -6.2912,  -7.7296,  -5.9373,  -7.9018,\n",
      "           -5.2587,  -5.1377,   0.1747],\n",
      "         [ -8.2304,  -5.5133,  -6.1738,  -7.8849,  -6.3725,  -7.0064,  -6.5564,\n",
      "           -6.7998,  -6.1222,   0.1744],\n",
      "         [ -7.4536,  -5.1456,  -5.0831,  -6.5862,  -6.8285,  -5.7041,  -6.6615,\n",
      "           -5.3391,  -4.5756,   0.1828],\n",
      "         [ -7.6126,  -5.7649,  -4.8852,  -5.2555,  -6.7094,  -6.1329,  -6.6661,\n",
      "           -4.9933,  -4.9044,   0.1777]]])\n",
      "tensor([[[-17.8769, -23.3927, -23.0364, -24.0974, -22.3587, -25.5619, -22.6394,\n",
      "          -24.3673, -22.9887,   0.1204],\n",
      "         [ -8.5753,  -5.0918,  -6.1946,  -7.9137,  -6.0418,  -7.5345,  -6.7446,\n",
      "           -6.6015,  -6.0907,   0.1781],\n",
      "         [ -8.1793,  -5.7422,  -4.6211,  -6.6215,  -6.1699,  -8.3161,  -5.1861,\n",
      "           -7.0347,  -6.5708,   0.1810],\n",
      "         [ -8.3630,  -4.9057,  -5.3062,  -7.1614,  -5.5305,  -7.8192,  -6.0188,\n",
      "           -6.5432,  -6.2156,   0.1791],\n",
      "         [ -7.9548,  -4.7458,  -4.6310,  -6.6888,  -5.7317,  -7.5327,  -5.8528,\n",
      "           -6.2643,  -5.8505,   0.1819],\n",
      "         [ -8.4949,  -5.1841,  -6.1956,  -7.7669,  -6.1367,  -7.2533,  -6.7078,\n",
      "           -6.4617,  -5.9481,   0.1761],\n",
      "         [ -8.3593,  -4.8783,  -5.7672,  -7.4795,  -5.9862,  -7.1658,  -6.4758,\n",
      "           -6.2749,  -5.7977,   0.1757],\n",
      "         [ -8.3177,  -6.4751,  -6.4255,  -6.2454,  -7.9196,  -5.7545,  -8.2058,\n",
      "           -5.0755,  -5.0194,   0.1743],\n",
      "         [ -8.3014,  -5.6246,  -6.3595,  -7.8511,  -6.5778,  -6.8346,  -6.8734,\n",
      "           -6.6276,  -6.0148,   0.1741],\n",
      "         [ -7.5190,  -5.2523,  -5.2659,  -6.5473,  -7.0276,  -5.5282,  -6.9738,\n",
      "           -5.1636,  -4.4661,   0.1826],\n",
      "         [ -7.6785,  -5.8697,  -5.0721,  -5.2250,  -6.9052,  -5.9574,  -6.9739,\n",
      "           -4.8212,  -4.7964,   0.1773],\n",
      "         [ -7.9298,  -5.3103,  -4.5012,  -6.3227,  -5.7062,  -7.7994,  -5.0695,\n",
      "           -6.5787,  -6.2079,   0.1763]]])\n",
      "tensor([[[-17.9308, -23.3287, -23.0455, -24.1674, -22.2847, -25.6643, -22.6167,\n",
      "          -24.4333, -23.0523,   0.1213],\n",
      "         [ -8.2386,  -5.7625,  -4.6819,  -6.7141,  -6.1842,  -8.4044,  -5.2292,\n",
      "           -7.1150,  -6.6418,   0.1812],\n",
      "         [ -8.4201,  -4.9251,  -5.3639,  -7.2505,  -5.5446,  -7.9047,  -6.0573,\n",
      "           -6.6206,  -6.2836,   0.1793],\n",
      "         [ -8.0150,  -4.7677,  -4.6902,  -6.7789,  -5.7476,  -7.6200,  -5.8934,\n",
      "           -6.3435,  -5.9215,   0.1821],\n",
      "         [ -8.5483,  -5.2014,  -6.2497,  -7.8524,  -6.1480,  -7.3361,  -6.7443,\n",
      "           -6.5362,  -6.0134,   0.1764],\n",
      "         [ -8.4152,  -4.8982,  -5.8236,  -7.5659,  -6.0001,  -7.2492,  -6.5136,\n",
      "           -6.3505,  -5.8646,   0.1759],\n",
      "         [ -8.3766,  -6.4937,  -6.4821,  -6.3355,  -7.9323,  -5.8422,  -8.2423,\n",
      "           -5.1553,  -5.0899,   0.1745],\n",
      "         [ -8.3536,  -5.6396,  -6.4117,  -7.9328,  -6.5868,  -6.9137,  -6.9066,\n",
      "           -6.6987,  -6.0773,   0.1744],\n",
      "         [ -7.5750,  -5.2712,  -5.3215,  -6.6326,  -7.0397,  -5.6112,  -7.0101,\n",
      "           -5.2394,  -4.5341,   0.1828],\n",
      "         [ -7.7356,  -5.8870,  -5.1267,  -5.3118,  -6.9169,  -6.0417,  -7.0089,\n",
      "           -4.8980,  -4.8646,   0.1774],\n",
      "         [ -7.9834,  -5.3238,  -4.5541,  -6.4056,  -5.7146,  -7.8777,  -5.1043,\n",
      "           -6.6497,  -6.2707,   0.1766],\n",
      "         [ -8.4388,  -5.1779,  -5.9393,  -7.3816,  -6.1361,  -7.0604,  -6.5964,\n",
      "           -6.2068,  -5.7519,   0.1824]]])\n",
      "tensor([[[-17.9530, -23.4751, -22.6792, -23.8908, -22.1777, -25.9899, -22.2270,\n",
      "          -24.6054, -23.2558,   0.1217],\n",
      "         [ -8.4798,  -5.0495,  -5.2810,  -7.2048,  -5.5889,  -8.0848,  -5.9737,\n",
      "           -6.7473,  -6.4130,   0.1799],\n",
      "         [ -8.0740,  -4.8965,  -4.6101,  -6.7351,  -5.7934,  -7.8059,  -5.8126,\n",
      "           -6.4744,  -6.0557,   0.1827],\n",
      "         [ -8.6111,  -5.3296,  -6.1742,  -7.8131,  -6.1993,  -7.5182,  -6.6722,\n",
      "           -6.6652,  -6.1465,   0.1771],\n",
      "         [ -8.4754,  -5.0249,  -5.7467,  -7.5237,  -6.0497,  -7.4275,  -6.4386,\n",
      "           -6.4760,  -5.9939,   0.1766],\n",
      "         [ -8.4498,  -6.6218,  -6.4117,  -6.3056,  -7.9854,  -6.0287,  -8.1726,\n",
      "           -5.2885,  -5.2248,   0.1751],\n",
      "         [ -8.4057,  -5.7567,  -6.3336,  -7.8902,  -6.6312,  -7.0852,  -6.8300,\n",
      "           -6.8188,  -6.2003,   0.1750],\n",
      "         [ -7.6362,  -5.3942,  -5.2483,  -6.5945,  -7.0861,  -5.7913,  -6.9391,\n",
      "           -5.3659,  -4.6626,   0.1837],\n",
      "         [ -7.8055,  -6.0077,  -5.0503,  -5.2740,  -6.9631,  -6.2216,  -6.9349,\n",
      "           -5.0248,  -4.9925,   0.1781],\n",
      "         [ -8.0335,  -5.4369,  -4.4668,  -6.3539,  -5.7509,  -8.0487,  -5.0192,\n",
      "           -6.7680,  -6.3917,   0.1773],\n",
      "         [ -8.4932,  -5.2926,  -5.8572,  -7.3349,  -6.1783,  -7.2285,  -6.5199,\n",
      "           -6.3226,  -5.8705,   0.1832],\n",
      "         [ -8.3150,  -5.1835,  -5.7658,  -7.3878,  -6.3084,  -6.9237,  -6.4802,\n",
      "           -6.3271,  -5.7470,   0.1790]]])\n",
      "0.21428571428571427\n",
      "micro average CR:  0.509651319407171\n",
      "macro average CR:  0.3411214953271028\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dsp = NeuralDispatch(net, encoder)\n",
    "metrics = get_batch_quality_metrics(dsp, 1, 50)\n",
    "print(get_CR(metrics))\n",
    "plot_CR(metrics[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'iter': 1, 'completed_orders': 0, 'finished_orders': 0, 'current_free_couriers': 5, 'current_free_orders': 1, 'current_active_routes': 3, 'total_eta': 8.836919973278874}\n",
      "{'iter': 2, 'completed_orders': 0, 'finished_orders': 0, 'current_free_couriers': 4, 'current_free_orders': 1, 'current_active_routes': 4, 'total_eta': 1.6155631948057896}\n",
      "{'iter': 3, 'completed_orders': 0, 'finished_orders': 0, 'current_free_couriers': 3, 'current_free_orders': 1, 'current_active_routes': 5, 'total_eta': 3.5197445030137233}\n",
      "{'iter': 4, 'completed_orders': 0, 'finished_orders': 0, 'current_free_couriers': 2, 'current_free_orders': 1, 'current_active_routes': 6, 'total_eta': 3.5947295007835267}\n",
      "{'iter': 5, 'completed_orders': 0, 'finished_orders': 0, 'current_free_couriers': 1, 'current_free_orders': 1, 'current_active_routes': 7, 'total_eta': 0.8350889572916034}\n",
      "{'iter': 6, 'completed_orders': 0, 'finished_orders': 0, 'current_free_couriers': 0, 'current_free_orders': 1, 'current_active_routes': 8, 'total_eta': 0.9077293110820376}\n",
      "{'iter': 7, 'completed_orders': 0, 'finished_orders': 0, 'current_free_couriers': 0, 'current_free_orders': 2, 'current_active_routes': 8, 'total_eta': 0}\n",
      "{'iter': 8, 'completed_orders': 0, 'finished_orders': 0, 'current_free_couriers': 0, 'current_free_orders': 3, 'current_active_routes': 8, 'total_eta': 0}\n",
      "{'iter': 9, 'completed_orders': 0, 'finished_orders': 0, 'current_free_couriers': 0, 'current_free_orders': 4, 'current_active_routes': 8, 'total_eta': 0}\n",
      "{'iter': 10, 'completed_orders': 1, 'finished_orders': 1, 'current_free_couriers': 1, 'current_free_orders': 5, 'current_active_routes': 7, 'total_eta': 0}\n",
      "{'iter': 11, 'completed_orders': 1, 'finished_orders': 1, 'current_free_couriers': 1, 'current_free_orders': 6, 'current_active_routes': 7, 'total_eta': 0}\n",
      "{'iter': 12, 'completed_orders': 1, 'finished_orders': 1, 'current_free_couriers': 1, 'current_free_orders': 7, 'current_active_routes': 7, 'total_eta': 0}\n",
      "{'iter': 13, 'completed_orders': 1, 'finished_orders': 1, 'current_free_couriers': 1, 'current_free_orders': 8, 'current_active_routes': 7, 'total_eta': 0}\n",
      "{'iter': 14, 'completed_orders': 2, 'finished_orders': 2, 'current_free_couriers': 2, 'current_free_orders': 9, 'current_active_routes': 6, 'total_eta': 0}\n",
      "{'iter': 15, 'completed_orders': 2, 'finished_orders': 2, 'current_free_couriers': 2, 'current_free_orders': 10, 'current_active_routes': 6, 'total_eta': 0}\n",
      "{'iter': 16, 'completed_orders': 2, 'finished_orders': 2, 'current_free_couriers': 2, 'current_free_orders': 11, 'current_active_routes': 6, 'total_eta': 0}\n",
      "{'iter': 17, 'completed_orders': 2, 'finished_orders': 3, 'current_free_couriers': 2, 'current_free_orders': 11, 'current_active_routes': 6, 'total_eta': 0}\n",
      "{'iter': 18, 'completed_orders': 3, 'finished_orders': 5, 'current_free_couriers': 3, 'current_free_orders': 11, 'current_active_routes': 5, 'total_eta': 0}\n",
      "{'iter': 19, 'completed_orders': 4, 'finished_orders': 7, 'current_free_couriers': 4, 'current_free_orders': 11, 'current_active_routes': 4, 'total_eta': 0}\n",
      "{'iter': 20, 'completed_orders': 4, 'finished_orders': 8, 'current_free_couriers': 4, 'current_free_orders': 11, 'current_active_routes': 4, 'total_eta': 0}\n",
      "{'iter': 21, 'completed_orders': 4, 'finished_orders': 9, 'current_free_couriers': 4, 'current_free_orders': 11, 'current_active_routes': 4, 'total_eta': 0}\n",
      "{'iter': 22, 'completed_orders': 6, 'finished_orders': 12, 'current_free_couriers': 6, 'current_free_orders': 11, 'current_active_routes': 2, 'total_eta': 0}\n",
      "{'iter': 23, 'completed_orders': 6, 'finished_orders': 13, 'current_free_couriers': 6, 'current_free_orders': 11, 'current_active_routes': 2, 'total_eta': 0}\n",
      "{'iter': 24, 'completed_orders': 6, 'finished_orders': 14, 'current_free_couriers': 6, 'current_free_orders': 11, 'current_active_routes': 2, 'total_eta': 0}\n",
      "{'iter': 25, 'completed_orders': 6, 'finished_orders': 15, 'current_free_couriers': 6, 'current_free_orders': 11, 'current_active_routes': 2, 'total_eta': 0}\n",
      "{'iter': 26, 'completed_orders': 7, 'finished_orders': 17, 'current_free_couriers': 7, 'current_free_orders': 11, 'current_active_routes': 1, 'total_eta': 0}\n",
      "{'iter': 27, 'completed_orders': 7, 'finished_orders': 18, 'current_free_couriers': 7, 'current_free_orders': 11, 'current_active_routes': 1, 'total_eta': 0}\n",
      "{'iter': 28, 'completed_orders': 7, 'finished_orders': 19, 'current_free_couriers': 7, 'current_free_orders': 11, 'current_active_routes': 1, 'total_eta': 0}\n",
      "{'iter': 29, 'completed_orders': 7, 'finished_orders': 20, 'current_free_couriers': 7, 'current_free_orders': 11, 'current_active_routes': 1, 'total_eta': 0}\n",
      "{'iter': 30, 'completed_orders': 7, 'finished_orders': 21, 'current_free_couriers': 7, 'current_free_orders': 11, 'current_active_routes': 1, 'total_eta': 0}\n",
      "{'iter': 31, 'completed_orders': 7, 'finished_orders': 22, 'current_free_couriers': 7, 'current_free_orders': 11, 'current_active_routes': 1, 'total_eta': 0}\n",
      "{'iter': 32, 'completed_orders': 7, 'finished_orders': 23, 'current_free_couriers': 7, 'current_free_orders': 11, 'current_active_routes': 1, 'total_eta': 0}\n",
      "{'iter': 33, 'completed_orders': 7, 'finished_orders': 24, 'current_free_couriers': 7, 'current_free_orders': 11, 'current_active_routes': 1, 'total_eta': 0}\n",
      "{'iter': 34, 'completed_orders': 7, 'finished_orders': 25, 'current_free_couriers': 7, 'current_free_orders': 11, 'current_active_routes': 1, 'total_eta': 0}\n",
      "{'iter': 35, 'completed_orders': 7, 'finished_orders': 26, 'current_free_couriers': 7, 'current_free_orders': 11, 'current_active_routes': 1, 'total_eta': 0}\n",
      "{'iter': 36, 'completed_orders': 8, 'finished_orders': 28, 'current_free_couriers': 8, 'current_free_orders': 11, 'current_active_routes': 0, 'total_eta': 0}\n",
      "{'iter': 37, 'completed_orders': 8, 'finished_orders': 29, 'current_free_couriers': 8, 'current_free_orders': 11, 'current_active_routes': 0, 'total_eta': 0}\n"
     ]
    }
   ],
   "source": [
    "for m in metrics[0][:37]:\n",
    "    # if m['current_free_couriers'] == 8:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'iter': 1,\n",
       "   'completed_orders': 0,\n",
       "   'finished_orders': 0,\n",
       "   'current_free_couriers': 5,\n",
       "   'current_free_orders': 1,\n",
       "   'current_active_routes': 3,\n",
       "   'total_eta': 8.836919973278874},\n",
       "  {'iter': 2,\n",
       "   'completed_orders': 0,\n",
       "   'finished_orders': 0,\n",
       "   'current_free_couriers': 4,\n",
       "   'current_free_orders': 1,\n",
       "   'current_active_routes': 4,\n",
       "   'total_eta': 1.6155631948057896},\n",
       "  {'iter': 3,\n",
       "   'completed_orders': 0,\n",
       "   'finished_orders': 0,\n",
       "   'current_free_couriers': 3,\n",
       "   'current_free_orders': 1,\n",
       "   'current_active_routes': 5,\n",
       "   'total_eta': 3.5197445030137233},\n",
       "  {'iter': 4,\n",
       "   'completed_orders': 0,\n",
       "   'finished_orders': 0,\n",
       "   'current_free_couriers': 2,\n",
       "   'current_free_orders': 1,\n",
       "   'current_active_routes': 6,\n",
       "   'total_eta': 3.5947295007835267},\n",
       "  {'iter': 5,\n",
       "   'completed_orders': 0,\n",
       "   'finished_orders': 0,\n",
       "   'current_free_couriers': 1,\n",
       "   'current_free_orders': 1,\n",
       "   'current_active_routes': 7,\n",
       "   'total_eta': 0.8350889572916034},\n",
       "  {'iter': 6,\n",
       "   'completed_orders': 0,\n",
       "   'finished_orders': 0,\n",
       "   'current_free_couriers': 0,\n",
       "   'current_free_orders': 1,\n",
       "   'current_active_routes': 8,\n",
       "   'total_eta': 0.9077293110820376},\n",
       "  {'iter': 7,\n",
       "   'completed_orders': 0,\n",
       "   'finished_orders': 0,\n",
       "   'current_free_couriers': 0,\n",
       "   'current_free_orders': 2,\n",
       "   'current_active_routes': 8,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 8,\n",
       "   'completed_orders': 0,\n",
       "   'finished_orders': 0,\n",
       "   'current_free_couriers': 0,\n",
       "   'current_free_orders': 3,\n",
       "   'current_active_routes': 8,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 9,\n",
       "   'completed_orders': 0,\n",
       "   'finished_orders': 0,\n",
       "   'current_free_couriers': 0,\n",
       "   'current_free_orders': 4,\n",
       "   'current_active_routes': 8,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 10,\n",
       "   'completed_orders': 1,\n",
       "   'finished_orders': 1,\n",
       "   'current_free_couriers': 1,\n",
       "   'current_free_orders': 5,\n",
       "   'current_active_routes': 7,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 11,\n",
       "   'completed_orders': 1,\n",
       "   'finished_orders': 1,\n",
       "   'current_free_couriers': 1,\n",
       "   'current_free_orders': 6,\n",
       "   'current_active_routes': 7,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 12,\n",
       "   'completed_orders': 1,\n",
       "   'finished_orders': 1,\n",
       "   'current_free_couriers': 1,\n",
       "   'current_free_orders': 7,\n",
       "   'current_active_routes': 7,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 13,\n",
       "   'completed_orders': 1,\n",
       "   'finished_orders': 1,\n",
       "   'current_free_couriers': 1,\n",
       "   'current_free_orders': 8,\n",
       "   'current_active_routes': 7,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 14,\n",
       "   'completed_orders': 2,\n",
       "   'finished_orders': 2,\n",
       "   'current_free_couriers': 2,\n",
       "   'current_free_orders': 9,\n",
       "   'current_active_routes': 6,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 15,\n",
       "   'completed_orders': 2,\n",
       "   'finished_orders': 2,\n",
       "   'current_free_couriers': 2,\n",
       "   'current_free_orders': 10,\n",
       "   'current_active_routes': 6,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 16,\n",
       "   'completed_orders': 2,\n",
       "   'finished_orders': 2,\n",
       "   'current_free_couriers': 2,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 6,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 17,\n",
       "   'completed_orders': 2,\n",
       "   'finished_orders': 3,\n",
       "   'current_free_couriers': 2,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 6,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 18,\n",
       "   'completed_orders': 3,\n",
       "   'finished_orders': 5,\n",
       "   'current_free_couriers': 3,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 5,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 19,\n",
       "   'completed_orders': 4,\n",
       "   'finished_orders': 7,\n",
       "   'current_free_couriers': 4,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 4,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 20,\n",
       "   'completed_orders': 4,\n",
       "   'finished_orders': 8,\n",
       "   'current_free_couriers': 4,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 4,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 21,\n",
       "   'completed_orders': 4,\n",
       "   'finished_orders': 9,\n",
       "   'current_free_couriers': 4,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 4,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 22,\n",
       "   'completed_orders': 6,\n",
       "   'finished_orders': 12,\n",
       "   'current_free_couriers': 6,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 2,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 23,\n",
       "   'completed_orders': 6,\n",
       "   'finished_orders': 13,\n",
       "   'current_free_couriers': 6,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 2,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 24,\n",
       "   'completed_orders': 6,\n",
       "   'finished_orders': 14,\n",
       "   'current_free_couriers': 6,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 2,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 25,\n",
       "   'completed_orders': 6,\n",
       "   'finished_orders': 15,\n",
       "   'current_free_couriers': 6,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 2,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 26,\n",
       "   'completed_orders': 7,\n",
       "   'finished_orders': 17,\n",
       "   'current_free_couriers': 7,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 1,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 27,\n",
       "   'completed_orders': 7,\n",
       "   'finished_orders': 18,\n",
       "   'current_free_couriers': 7,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 1,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 28,\n",
       "   'completed_orders': 7,\n",
       "   'finished_orders': 19,\n",
       "   'current_free_couriers': 7,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 1,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 29,\n",
       "   'completed_orders': 7,\n",
       "   'finished_orders': 20,\n",
       "   'current_free_couriers': 7,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 1,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 30,\n",
       "   'completed_orders': 7,\n",
       "   'finished_orders': 21,\n",
       "   'current_free_couriers': 7,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 1,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 31,\n",
       "   'completed_orders': 7,\n",
       "   'finished_orders': 22,\n",
       "   'current_free_couriers': 7,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 1,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 32,\n",
       "   'completed_orders': 7,\n",
       "   'finished_orders': 23,\n",
       "   'current_free_couriers': 7,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 1,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 33,\n",
       "   'completed_orders': 7,\n",
       "   'finished_orders': 24,\n",
       "   'current_free_couriers': 7,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 1,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 34,\n",
       "   'completed_orders': 7,\n",
       "   'finished_orders': 25,\n",
       "   'current_free_couriers': 7,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 1,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 35,\n",
       "   'completed_orders': 7,\n",
       "   'finished_orders': 26,\n",
       "   'current_free_couriers': 7,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 1,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 36,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 28,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 37,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 29,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 38,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 30,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 39,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 31,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 40,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 32,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 41,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 33,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 42,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 34,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 43,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 35,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 44,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 36,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 45,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 37,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 46,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 38,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 47,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 39,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 48,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 40,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 49,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 41,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 50,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 42,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 51,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 43,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 52,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 44,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 53,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 45,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 54,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 46,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 55,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 47,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 56,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 48,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 57,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 49,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 58,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 50,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 59,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 51,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 60,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 52,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 61,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 53,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 62,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 54,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 63,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 55,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 64,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 56,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 65,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 57,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 66,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 58,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 67,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 59,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 68,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 60,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 69,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 61,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 70,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 62,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 71,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 63,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 72,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 64,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 73,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 65,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 74,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 66,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 75,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 67,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 76,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 68,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 77,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 69,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 78,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 70,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 79,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 71,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 80,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 72,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 81,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 73,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 82,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 74,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 83,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 75,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 84,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 76,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 85,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 77,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 86,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 78,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 87,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 79,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 88,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 80,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 89,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 81,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 90,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 82,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 91,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 83,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 92,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 84,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 93,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 85,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 94,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 86,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 95,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 87,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 96,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 88,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 97,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 89,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 98,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 90,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 99,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 91,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 100,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 92,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 101,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 93,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 102,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 94,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 103,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 95,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 104,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 96,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 105,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 97,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 106,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 98,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 107,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 99,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 108,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 100,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 109,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 101,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 110,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 102,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 111,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 103,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 112,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 104,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 113,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 105,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 114,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 106,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 115,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 107,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 116,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 108,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 117,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 109,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 118,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 110,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 119,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 111,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 120,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 112,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 121,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 113,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 122,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 114,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 123,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 115,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 124,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 116,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 125,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 117,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 126,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 118,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 127,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 119,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 128,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 120,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 129,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 121,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 130,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 122,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 131,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 123,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 132,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 124,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 133,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 125,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 134,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 126,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 135,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 127,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 136,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 128,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 137,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 129,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 138,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 130,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 139,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 131,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 140,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 132,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 141,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 133,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 142,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 134,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 143,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 135,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 144,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 136,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 145,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 137,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 146,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 138,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 147,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 139,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 148,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 140,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 149,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 141,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 150,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 142,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 151,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 143,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 152,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 144,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 153,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 145,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 154,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 146,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 155,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 147,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 156,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 148,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 157,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 149,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 158,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 150,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 159,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 151,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 160,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 152,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 161,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 153,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 162,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 154,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 163,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 155,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 164,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 156,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 165,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 157,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 166,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 158,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 167,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 159,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 168,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 160,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 169,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 161,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 170,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 162,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 171,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 163,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 172,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 164,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 173,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 165,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 174,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 166,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 175,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 167,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 176,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 168,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 177,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 169,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 178,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 170,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 179,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 171,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 180,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 172,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 181,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 173,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 182,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 174,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 183,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 175,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 184,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 176,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 185,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 177,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 186,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 178,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 187,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 179,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 188,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 180,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 189,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 181,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 190,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 182,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 191,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 183,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 192,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 184,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 193,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 185,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 194,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 186,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 195,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 187,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 196,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 188,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 197,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 189,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 198,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 190,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 199,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 191,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 200,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 192,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 201,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 193,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 202,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 194,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 203,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 195,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 204,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 196,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 205,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 197,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 206,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 198,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 207,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 199,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 208,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 200,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 209,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 201,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 210,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 202,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 211,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 203,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 212,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 204,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 213,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 205,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 214,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 206,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 215,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 207,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 216,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 208,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 217,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 209,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 218,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 210,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 219,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 211,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 220,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 212,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 221,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 213,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 222,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 214,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 223,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 215,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 224,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 216,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 225,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 217,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 226,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 218,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 227,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 219,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 228,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 220,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 229,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 221,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 230,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 222,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 231,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 223,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 232,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 224,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 233,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 225,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 234,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 226,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 235,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 227,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 236,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 228,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 237,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 229,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 238,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 230,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 239,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 231,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 240,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 232,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 241,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 233,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 242,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 234,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 243,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 235,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 244,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 236,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 245,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 237,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 246,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 238,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 247,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 239,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 248,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 240,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 249,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 241,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 250,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 242,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 251,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 243,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 252,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 244,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 253,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 245,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 254,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 246,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 255,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 247,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 256,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 248,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 257,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 249,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 258,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 250,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 259,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 251,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 260,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 252,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 261,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 253,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 262,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 254,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 263,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 255,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 264,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 256,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 265,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 257,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 266,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 258,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 267,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 259,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 268,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 260,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 269,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 261,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 270,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 262,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 271,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 263,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 272,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 264,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 273,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 265,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 274,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 266,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 275,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 267,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 276,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 268,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 277,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 269,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 278,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 270,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 279,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 271,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 280,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 272,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 281,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 273,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 282,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 274,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 283,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 275,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 284,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 276,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 285,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 277,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 286,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 278,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 287,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 279,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 288,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 280,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 289,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 281,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 290,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 282,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 291,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 283,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 292,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 284,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 293,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 285,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 294,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 286,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 295,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 287,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 296,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 288,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 297,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 289,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 298,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 290,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 299,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 291,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 300,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 292,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 301,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 293,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 302,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 294,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 303,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 295,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 304,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 296,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 305,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 297,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 306,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 298,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 307,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 299,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 308,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 300,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 309,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 301,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 310,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 302,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 311,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 303,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 312,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 304,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 313,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 305,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 314,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 306,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 315,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 307,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 316,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 308,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 317,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 309,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 318,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 310,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 319,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 311,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 320,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 312,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 321,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 313,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 322,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 314,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 323,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 315,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 324,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 316,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 325,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 317,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 326,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 318,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 327,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 319,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 328,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 320,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 329,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 321,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 330,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 322,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 331,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 323,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 332,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 324,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 333,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 325,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 334,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 326,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 335,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 327,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 336,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 328,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 337,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 329,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 338,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 330,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 339,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 331,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 340,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 332,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 341,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 333,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 342,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 334,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 343,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 335,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 344,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 336,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 345,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 337,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 346,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 338,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 347,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 339,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 348,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 340,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 349,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 341,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 350,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 342,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 351,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 343,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 352,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 344,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 353,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 345,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 354,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 346,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 355,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 347,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 356,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 348,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 357,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 349,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 358,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 350,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 359,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 351,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 360,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 352,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 361,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 353,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 362,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 354,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 363,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 355,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 364,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 356,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 365,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 357,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 366,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 358,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 367,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 359,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 368,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 360,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 369,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 361,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 370,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 362,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 371,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 363,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 372,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 364,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 373,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 365,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 374,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 366,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 375,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 367,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 376,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 368,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 377,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 369,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 378,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 370,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 379,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 371,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 380,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 372,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 381,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 373,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 382,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 374,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 383,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 375,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 384,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 376,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 385,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 377,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 386,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 378,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 387,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 379,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 388,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 380,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 389,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 381,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 390,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 382,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 391,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 383,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 392,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 384,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 393,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 385,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 394,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 386,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 395,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 387,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 396,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 388,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 397,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 389,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 398,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 390,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 399,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 391,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 400,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 392,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 401,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 393,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 402,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 394,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 403,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 395,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 404,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 396,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 405,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 397,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 406,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 398,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 407,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 399,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 408,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 400,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 409,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 401,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 410,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 402,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 411,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 403,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 412,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 404,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 413,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 405,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 414,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 406,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 415,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 407,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 416,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 408,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 417,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 409,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 418,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 410,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 419,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 411,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 420,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 412,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 421,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 413,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 422,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 414,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 423,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 415,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 424,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 416,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 425,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 417,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 426,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 418,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 427,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 419,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 428,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 420,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 429,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 421,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 430,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 422,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 431,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 423,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 432,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 424,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 433,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 425,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 434,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 426,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 435,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 427,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 436,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 428,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 437,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 429,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 438,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 430,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 439,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 431,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 440,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 432,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 441,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 433,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 442,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 434,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 443,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 435,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 444,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 436,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 445,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 437,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 446,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 438,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 447,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 439,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 448,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 440,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 449,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 441,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 450,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 442,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 451,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 443,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 452,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 444,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 453,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 445,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 454,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 446,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 455,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 447,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 456,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 448,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 457,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 449,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 458,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 450,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 459,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 451,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 460,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 452,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 461,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 453,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 462,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 454,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 463,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 455,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 464,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 456,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 465,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 457,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 466,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 458,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 467,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 459,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 468,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 460,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 469,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 461,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 470,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 462,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 471,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 463,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 472,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 464,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 473,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 465,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 474,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 466,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 475,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 467,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 476,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 468,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 477,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 469,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 478,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 470,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 479,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 471,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 480,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 472,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 481,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 473,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 482,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 474,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 483,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 475,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 484,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 476,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 485,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 477,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 486,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 478,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 487,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 479,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 488,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 480,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 489,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 481,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 490,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 482,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 491,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 483,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 492,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 484,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 493,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 485,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 494,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 486,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 495,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 487,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 496,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 488,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 497,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 489,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 498,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 490,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 499,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 491,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0},\n",
       "  {'iter': 500,\n",
       "   'completed_orders': 8,\n",
       "   'finished_orders': 492,\n",
       "   'current_free_couriers': 8,\n",
       "   'current_free_orders': 11,\n",
       "   'current_active_routes': 0,\n",
       "   'total_eta': 0}]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL field "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tensordict.nn import TensorDictModule\n",
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchrl.collectors import SyncDataCollector, MultiaSyncDataCollector, MultiSyncDataCollector\n",
    "from torchrl.data.replay_buffers import ReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "from torchrl.envs import (\n",
    "    Compose,\n",
    "    DoubleToFloat,\n",
    "    ObservationNorm,\n",
    "    StepCounter,\n",
    "    TransformedEnv,\n",
    ")\n",
    "from torchrl.envs.libs.gym import GymEnv\n",
    "from torchrl.envs.utils import check_env_specs, set_exploration_mode\n",
    "from torchrl.modules import ProbabilisticActor, TanhNormal, ValueOperator\n",
    "from torchrl.objectives import ClipPPOLoss\n",
    "from torchrl.objectives.value import GAE\n",
    "from torchrl.envs import EnvBase\n",
    "from tensordict.tensordict import TensorDictBase, TensorDict\n",
    "from typing import Optional\n",
    "import gym\n",
    "from torchrl.data import CompositeSpec, BoundedTensorSpec, UnboundedContinuousTensorSpec, BinaryDiscreteTensorSpec, OneHotDiscreteTensorSpec, DiscreteTensorSpec, UnboundedDiscreteTensorSpec\n",
    "\n",
    "class SimulatorEnv(EnvBase):\n",
    "    def __init__(self, simulator: type[Simulator], seed=None, device=\"cpu\"):\n",
    "        super().__init__(device=device, batch_size=[])\n",
    "\n",
    "        self.load_settings()\n",
    "        \n",
    "        # self.simulators = [simulator(seed=i) for i in range(sub_batch_size)]\n",
    "        self.simulator = simulator()\n",
    "        self.encoder = GambleTripleEncoder(number_enc_dim=self.number_enc_dim, d_model=self.d_model, point_enc_dim=self.point_enc_dim)\n",
    "        # self.model = ScoringInterface(net)\n",
    "        # triples = [simulator.GetState() for simulator in self.simulators]\n",
    "        # self.model.encode_input(triples, 0)\n",
    "        self._make_specs()\n",
    "        \n",
    "        if seed is None:\n",
    "            seed = torch.empty((), dtype=torch.long).random_().item()\n",
    "        self.set_seed(seed)\n",
    "\n",
    "    def load_settings(self):\n",
    "        with open('configs/rl_settings.json') as f:\n",
    "            settings = json.load(f)\n",
    "        self.max_num_orders = settings['max_num_orders']\n",
    "        self.max_num_couriers = settings['max_num_couriers']\n",
    "        self.max_num_active_routes = settings['max_num_active_routes']\n",
    "\n",
    "        with open('configs/network_hyperparams.json') as f:\n",
    "            hyperparams = json.load(f)\n",
    "        self.number_enc_dim = hyperparams['number_enc_dim']\n",
    "        self.d_model = hyperparams['d_model']\n",
    "        self.point_enc_dim = hyperparams['point_enc_dim']\n",
    "\n",
    "\n",
    "    def make_masks(self, tensors):\n",
    "        masks = {\n",
    "            'o': torch.tensor([True] + [False] * (len(tensors['o']) - 1), device=device, dtype=torch.bool),\n",
    "            'c': torch.tensor([True] + [False] * (len(tensors['c']) - 1), device=device, dtype=torch.bool),\n",
    "            'ar': torch.tensor([True] + [False] * (len(tensors['ar']) - 1), device=device, dtype=torch.bool)\n",
    "        }\n",
    "\n",
    "        return masks\n",
    "    \n",
    "    def pad_tensors(self, tensors, masks, ids):\n",
    "        '''\n",
    "        Pads tensors to max_limits inplace\n",
    "        '''\n",
    "        max_limits = {\n",
    "            'o': self.max_num_orders,\n",
    "            'c': self.max_num_couriers,\n",
    "            'ar': self.max_num_active_routes\n",
    "        }\n",
    "        for item_type in ['o', 'c', 'ar']:\n",
    "            length = tensors[item_type].shape[0]\n",
    "            tensors[item_type] = F.pad(input=tensors[item_type], pad=(0, 0, 0, max_limits[item_type] - length), mode='constant', value=0.0)\n",
    "            masks[item_type] = F.pad(input=masks[item_type], pad=(0, max_limits[item_type] - length), mode='constant', value=True)\n",
    "            ids[item_type] = F.pad(input=ids[item_type], pad=(0, max_limits[item_type] - length), mode='constant', value=-1)\n",
    "\n",
    "        \n",
    "    def _step(self, tensordict: TensorDictBase) -> TensorDictBase:\n",
    "        '''\n",
    "        tensordict['action'] - a np.array of indexes (not IDs) of couriers assigned for the given order. If there is no courier assigned -1 is provided.\n",
    "        BOS-fake items are included.\n",
    "        '''\n",
    "\n",
    "        assignments = []\n",
    "        assigned_o_idxs = set()\n",
    "        assigned_c_idxs = set()\n",
    "        for o_idx, c_idx in enumerate(tensordict['action'].numpy()):\n",
    "            if c_idx != - 1 \\\n",
    "                and not tensordict['observation', 'masks', 'o'][o_idx] \\\n",
    "                and not tensordict['observation', 'masks', 'c'][c_idx] \\\n",
    "                and (o_idx not in assigned_o_idxs) and (c_idx not in assigned_c_idxs) \\\n",
    "            :\n",
    "                assignment = (tensordict['observation', 'ids', 'o'][o_idx].item(), tensordict['observation', 'ids', 'c'][c_idx].item())\n",
    "                assignments.append(assignment)\n",
    "                assigned_o_idxs.add(o_idx)\n",
    "                assigned_c_idxs.add(c_idx)\n",
    "\n",
    "        # print(assignments)\n",
    "        # print(self.simulator.GetState())\n",
    "        self.simulator.Next(assignments)\n",
    "        triple = self.simulator.GetState()\n",
    "        tensors, ids = self.encoder(triple, 0)\n",
    "        masks = self.make_masks(tensors)\n",
    "        self.pad_tensors(tensors, masks, ids)\n",
    "\n",
    "        out = TensorDict(\n",
    "            {\n",
    "                \"next\": {\n",
    "                    \"observation\": {\n",
    "                        'tensors': {\n",
    "                            'o': tensors['o'],\n",
    "                            'c': tensors['c'],\n",
    "                            'ar': tensors['ar']     \n",
    "                        },\n",
    "                        'masks': {\n",
    "                            'o': masks['o'],\n",
    "                            'c': masks['c'],\n",
    "                            'ar': masks['ar']     \n",
    "                        },\n",
    "                        'ids': {\n",
    "                            'o': ids['o'],\n",
    "                            'c': ids['c'],\n",
    "                            'ar': ids['ar']\n",
    "                        }\n",
    "                    },\n",
    "                    \"reward\": torch.tensor(0, dtype=torch.float32),\n",
    "                    \"done\": torch.tensor(False, dtype=torch.bool),\n",
    "                    # \"reward\": torch.tensor([0] * sub_batch_size, dtype=torch.float32),\n",
    "                    # \"done\": torch.tensor([False] * sub_batch_size, dtype=torch.bool),\n",
    "                }\n",
    "            },\n",
    "            tensordict.shape\n",
    "            # batch_size=tensordict.shape[0]\n",
    "        )\n",
    "        # print(out['next', 'observation', 'ids', 'o'])\n",
    "        return out\n",
    "    \n",
    "    def _reset(self, tensordict: TensorDictBase) -> TensorDictBase:\n",
    "        if tensordict is None or tensordict.is_empty():\n",
    "            triple = self.simulator.GetState()\n",
    "            tensors, ids = self.encoder(triple, 0)\n",
    "            masks = self.make_masks(tensors)\n",
    "            self.pad_tensors(tensors, masks, ids)\n",
    "            \n",
    "            return TensorDict(\n",
    "            {\n",
    "                \"observation\": {\n",
    "                    'tensors': {\n",
    "                        'o': tensors['o'],\n",
    "                        'c': tensors['c'],\n",
    "                        'ar': tensors['ar']     \n",
    "                    },\n",
    "                    'masks': {\n",
    "                        'o': masks['o'],\n",
    "                        'c': masks['c'],\n",
    "                        'ar': masks['ar']     \n",
    "                    },\n",
    "                    'ids': {\n",
    "                        'o': ids['o'],\n",
    "                        'c': ids['c'],\n",
    "                        'ar': ids['ar']\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            # batch_size=[sub_batch_size]\n",
    "            batch_size=self.batch_size\n",
    "        ) \n",
    "        return tensordict\n",
    "    \n",
    "    def _set_seed(self, seed: Optional[int]):\n",
    "        rng = torch.manual_seed(seed)\n",
    "        self.rng = rng\n",
    "\n",
    "    def _make_specs(self) -> None:\n",
    "        self.action_spec = DiscreteTensorSpec(\n",
    "            n=self.max_num_couriers,\n",
    "            dtype=torch.int,\n",
    "            shape=[self.max_num_orders]\n",
    "        )\n",
    "        observation_spec = CompositeSpec(\n",
    "            tensors = CompositeSpec(\n",
    "                o = UnboundedContinuousTensorSpec(\n",
    "                    shape=[self.max_num_orders, self.encoder.d_model],\n",
    "                    dtype=torch.float\n",
    "                ),\n",
    "                c = UnboundedContinuousTensorSpec(\n",
    "                    shape=[self.max_num_couriers, self.encoder.d_model],\n",
    "                    dtype=torch.float\n",
    "                ),\n",
    "                ar = UnboundedContinuousTensorSpec(\n",
    "                    shape=[self.max_num_active_routes, self.encoder.d_model],\n",
    "                    dtype=torch.float\n",
    "                ),\n",
    "                # shape=[sub_batch_size]\n",
    "            ),\n",
    "            masks = CompositeSpec(\n",
    "                o = DiscreteTensorSpec(\n",
    "                    n=2,\n",
    "                    dtype=torch.bool,\n",
    "                    shape=[self.max_num_orders]\n",
    "                ),\n",
    "                c = DiscreteTensorSpec(\n",
    "                    n=2,\n",
    "                    dtype=torch.bool,\n",
    "                    shape=[self.max_num_couriers]\n",
    "                ),\n",
    "                ar = DiscreteTensorSpec(\n",
    "                    n=2,\n",
    "                    dtype=torch.bool,\n",
    "                    shape=[self.max_num_active_routes]\n",
    "                ),\n",
    "                # shape=[sub_batch_size]\n",
    "            ),\n",
    "            ids = CompositeSpec(\n",
    "                o = UnboundedDiscreteTensorSpec(\n",
    "                    dtype=torch.int,\n",
    "                    shape=[self.max_num_orders]\n",
    "                ),\n",
    "                c = UnboundedDiscreteTensorSpec(\n",
    "                    dtype=torch.int,\n",
    "                    shape=[self.max_num_couriers]\n",
    "                ),\n",
    "                ar = UnboundedDiscreteTensorSpec(\n",
    "                    dtype=torch.int,\n",
    "                    shape=[self.max_num_active_routes]\n",
    "                ),\n",
    "                # shape=[sub_batch_size]\n",
    "            ), \n",
    "            # shape=[sub_batch_size]\n",
    "        )\n",
    "        # if not isinstance(observation_spec, CompositeSpec):\n",
    "        observation_spec = CompositeSpec(observation=observation_spec) # shape=[sub_batch_size]\n",
    "            \n",
    "        self.observation_spec = observation_spec\n",
    "        self.reward_spec = UnboundedContinuousTensorSpec(\n",
    "            # shape=[sub_batch_size],\n",
    "            shape=[1],\n",
    "            dtype=torch.float32,\n",
    "        )\n",
    "        self.done_spec = BinaryDiscreteTensorSpec(\n",
    "            # n=sub_batch_size,\n",
    "            # shape=[sub_batch_size],\n",
    "            n=1,\n",
    "            shape=[1],\n",
    "            dtype=torch.bool\n",
    "        )\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_env = SimulatorEnv(Simulator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TransformedEnv(\n",
    "    my_env,\n",
    "    # Compose(\n",
    "    #     # normalize observations\n",
    "    #     # ObservationNorm(in_keys=[\"observation\"]),\n",
    "    #     # DoubleToFloat(in_keys=[\"observation\"]),\n",
    "    #     StepCounter(),\n",
    "    # ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation_spec: CompositeSpec(\n",
      "    observation: CompositeSpec(\n",
      "        tensors: CompositeSpec(\n",
      "            o: UnboundedContinuousTensorSpec(\n",
      "                 shape=torch.Size([100, 512]), space=None, device=cpu, dtype=torch.float32, domain=continuous),\n",
      "            c: UnboundedContinuousTensorSpec(\n",
      "                 shape=torch.Size([100, 512]), space=None, device=cpu, dtype=torch.float32, domain=continuous),\n",
      "            ar: UnboundedContinuousTensorSpec(\n",
      "                 shape=torch.Size([100, 512]), space=None, device=cpu, dtype=torch.float32, domain=continuous), device=cpu, shape=torch.Size([])),\n",
      "        masks: CompositeSpec(\n",
      "            o: DiscreteTensorSpec(\n",
      "                 shape=torch.Size([100]), space=DiscreteBox(n=2), device=cpu, dtype=torch.bool, domain=discrete),\n",
      "            c: DiscreteTensorSpec(\n",
      "                 shape=torch.Size([100]), space=DiscreteBox(n=2), device=cpu, dtype=torch.bool, domain=discrete),\n",
      "            ar: DiscreteTensorSpec(\n",
      "                 shape=torch.Size([100]), space=DiscreteBox(n=2), device=cpu, dtype=torch.bool, domain=discrete), device=cpu, shape=torch.Size([])),\n",
      "        ids: CompositeSpec(\n",
      "            o: UnboundedDiscreteTensorSpec(\n",
      "                 shape=torch.Size([100]), space=ContinuousBox(minimum=Tensor(shape=torch.Size([100]), device=cpu, dtype=torch.int64, contiguous=True), maximum=Tensor(shape=torch.Size([100]), device=cpu, dtype=torch.int64, contiguous=True)), device=cpu, dtype=torch.int32, domain=continuous),\n",
      "            c: UnboundedDiscreteTensorSpec(\n",
      "                 shape=torch.Size([100]), space=ContinuousBox(minimum=Tensor(shape=torch.Size([100]), device=cpu, dtype=torch.int64, contiguous=True), maximum=Tensor(shape=torch.Size([100]), device=cpu, dtype=torch.int64, contiguous=True)), device=cpu, dtype=torch.int32, domain=continuous),\n",
      "            ar: UnboundedDiscreteTensorSpec(\n",
      "                 shape=torch.Size([100]), space=ContinuousBox(minimum=Tensor(shape=torch.Size([100]), device=cpu, dtype=torch.int64, contiguous=True), maximum=Tensor(shape=torch.Size([100]), device=cpu, dtype=torch.int64, contiguous=True)), device=cpu, dtype=torch.int32, domain=continuous), device=cpu, shape=torch.Size([])), device=cpu, shape=torch.Size([])), device=cpu, shape=torch.Size([]))\n",
      "reward_spec: UnboundedContinuousTensorSpec(\n",
      "     shape=torch.Size([1]), space=None, device=cpu, dtype=torch.float32, domain=continuous)\n",
      "input_spec: CompositeSpec(\n",
      "    action: DiscreteTensorSpec(\n",
      "         shape=torch.Size([100]), space=DiscreteBox(n=100), device=cpu, dtype=torch.int32, domain=discrete), device=cpu, shape=torch.Size([]))\n",
      "action_spec (as defined by input_spec): DiscreteTensorSpec(\n",
      "     shape=torch.Size([100]), space=DiscreteBox(n=100), device=cpu, dtype=torch.int32, domain=discrete)\n"
     ]
    }
   ],
   "source": [
    "# env.transform[0].init_stats(num_iter=1000, reduce_dim=0, cat_dim=0)\n",
    "# print(\"normalization constant shape:\", env.transform[0].loc.shape)\n",
    "print(\"observation_spec:\", my_env.observation_spec)\n",
    "print(\"reward_spec:\", my_env.reward_spec)\n",
    "print(\"input_spec:\", my_env.input_spec)\n",
    "print(\"action_spec (as defined by input_spec):\", my_env.action_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_env_specs succeeded!\n"
     ]
    }
   ],
   "source": [
    "check_env_specs(my_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rollout of three steps: TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([10, 100]), device=cpu, dtype=torch.int32, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                observation: TensorDict(\n",
      "                    fields={\n",
      "                        ids: TensorDict(\n",
      "                            fields={\n",
      "                                ar: Tensor(shape=torch.Size([10, 100]), device=cpu, dtype=torch.int32, is_shared=False),\n",
      "                                c: Tensor(shape=torch.Size([10, 100]), device=cpu, dtype=torch.int32, is_shared=False),\n",
      "                                o: Tensor(shape=torch.Size([10, 100]), device=cpu, dtype=torch.int32, is_shared=False)},\n",
      "                            batch_size=torch.Size([10]),\n",
      "                            device=cpu,\n",
      "                            is_shared=False),\n",
      "                        masks: TensorDict(\n",
      "                            fields={\n",
      "                                ar: Tensor(shape=torch.Size([10, 100]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                                c: Tensor(shape=torch.Size([10, 100]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                                o: Tensor(shape=torch.Size([10, 100]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "                            batch_size=torch.Size([10]),\n",
      "                            device=cpu,\n",
      "                            is_shared=False),\n",
      "                        tensors: TensorDict(\n",
      "                            fields={\n",
      "                                ar: Tensor(shape=torch.Size([10, 100, 512]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                                c: Tensor(shape=torch.Size([10, 100, 512]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                                o: Tensor(shape=torch.Size([10, 100, 512]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "                            batch_size=torch.Size([10]),\n",
      "                            device=cpu,\n",
      "                            is_shared=False)},\n",
      "                    batch_size=torch.Size([10]),\n",
      "                    device=cpu,\n",
      "                    is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "            batch_size=torch.Size([10]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        observation: TensorDict(\n",
      "            fields={\n",
      "                ids: TensorDict(\n",
      "                    fields={\n",
      "                        ar: Tensor(shape=torch.Size([10, 100]), device=cpu, dtype=torch.int32, is_shared=False),\n",
      "                        c: Tensor(shape=torch.Size([10, 100]), device=cpu, dtype=torch.int32, is_shared=False),\n",
      "                        o: Tensor(shape=torch.Size([10, 100]), device=cpu, dtype=torch.int32, is_shared=False)},\n",
      "                    batch_size=torch.Size([10]),\n",
      "                    device=cpu,\n",
      "                    is_shared=False),\n",
      "                masks: TensorDict(\n",
      "                    fields={\n",
      "                        ar: Tensor(shape=torch.Size([10, 100]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                        c: Tensor(shape=torch.Size([10, 100]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                        o: Tensor(shape=torch.Size([10, 100]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "                    batch_size=torch.Size([10]),\n",
      "                    device=cpu,\n",
      "                    is_shared=False),\n",
      "                tensors: TensorDict(\n",
      "                    fields={\n",
      "                        ar: Tensor(shape=torch.Size([10, 100, 512]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        c: Tensor(shape=torch.Size([10, 100, 512]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        o: Tensor(shape=torch.Size([10, 100, 512]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "                    batch_size=torch.Size([10]),\n",
      "                    device=cpu,\n",
      "                    is_shared=False)},\n",
      "            batch_size=torch.Size([10]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "    batch_size=torch.Size([10]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n",
      "Shape of the rollout TensorDict: torch.Size([10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rollout = my_env.rollout(10)\n",
    "print(\"rollout of three steps:\", rollout)\n",
    "print(\"Shape of the rollout TensorDict:\", rollout.batch_size)\n",
    "rollout.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = TensorDictModule(\n",
    "    net, in_keys=[('observation', 'tensors'), ('observation', 'masks')], out_keys=['logits', 'values']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# td = my_env.reset()\n",
    "# net(td['observation', 'tensors'].to_dict(), td['observation', 'masks'].to_dict())\n",
    "# td['observation', 'tensors'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running policy: torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "print(\"Running policy:\", module(my_env.reset()).shape)\n",
    "# env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in [0, 1, 2, 3]:\n",
    "#     print(my_env.simulators[i].GetMetrics())\n",
    "#     print('-'* 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensordict.nn.distributions import Categorical\n",
    "from torch.distributions.categorical import Categorical\n",
    "policy_module_actor = ProbabilisticActor(\n",
    "    module=module,\n",
    "    in_keys=[\"logits\"],\n",
    "    distribution_class=Categorical,\n",
    "    # distribution_kwargs={\n",
    "    #     \"n\": env.action_spec.space.n,\n",
    "    # },\n",
    "    return_log_prob=True,\n",
    "    # we'll need the log-prob for the numerator of the importance weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogPolicyAvg(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input, mask):\n",
    "        assert (len(input.shape) == 2 and len(mask.shape) == 2) or (len(input.shape) == 1 and len(mask.shape) == 1), 'dims should be [bs, ord] or [ord]'\n",
    "\n",
    "        sums = torch.sum(torch.where(mask, input, 0), dim=-1)\n",
    "        nums = torch.sum(torch.where(mask, 1, 0), dim=-1)\n",
    "\n",
    "        return torch.where(nums > 0, sums / nums, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensordict.nn import TensorDictModule, TensorDictSequential\n",
    "final_module = TensorDictSequential(\n",
    "    policy_module_actor,\n",
    "    TensorDictModule(LogPolicyAvg(), in_keys=['sample_log_prob', ('observation', 'masks', 'o')], out_keys=['sample_log_prob']),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value_module = ValueOperator(\n",
    "#     module=net,\n",
    "#     in_keys=[('observation', 'tensors'), ('observation', 'masks')], \n",
    "#     out_keys=['logits', 'values']\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "collector = SyncDataCollector(\n",
    "    my_env,\n",
    "    policy_module_actor,\n",
    "    frames_per_batch=1,\n",
    "    total_frames=5,\n",
    "    split_trajs=False,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 100 is out of bounds for dimension 0 with size 100",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/dbok/PycharmProjects/delivery-RL/delivery-RL/rl_training.ipynb Cell 26\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/dbok/PycharmProjects/delivery-RL/delivery-RL/rl_training.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, c \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(collector):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dbok/PycharmProjects/delivery-RL/delivery-RL/rl_training.ipynb#X34sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dbok/PycharmProjects/delivery-RL/delivery-RL/rl_training.ipynb#X34sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torchrl/collectors/collectors.py:596\u001b[0m, in \u001b[0;36mSyncDataCollector.iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    594\u001b[0m i \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    595\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter \u001b[39m=\u001b[39m i\n\u001b[0;32m--> 596\u001b[0m tensordict_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrollout()\n\u001b[1;32m    597\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_frames \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tensordict_out\u001b[39m.\u001b[39mnumel()\n\u001b[1;32m    598\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_frames \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m total_frames:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torchrl/_utils.py:296\u001b[0m, in \u001b[0;36maccept_remote_rref_invocation.<locals>.unpack_rref_and_invoke_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _os_is_windows \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m, torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_distributed_rpc\u001b[39m.\u001b[39mPyRRef):\n\u001b[1;32m    295\u001b[0m     \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlocal_value()\n\u001b[0;32m--> 296\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torchrl/collectors/collectors.py:687\u001b[0m, in \u001b[0;36mSyncDataCollector.rollout\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    685\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mrand_step(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tensordict)\n\u001b[1;32m    686\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 687\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tensordict))\n\u001b[1;32m    689\u001b[0m \u001b[39m# we must clone all the values, since the step / traj_id updates are done in-place\u001b[39;00m\n\u001b[1;32m    690\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torchrl/envs/common.py:371\u001b[0m, in \u001b[0;36mEnvBase.step\u001b[0;34m(self, tensordict)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_assert_tensordict_shape(tensordict)\n\u001b[1;32m    370\u001b[0m tensordict\u001b[39m.\u001b[39mlock_()  \u001b[39m# make sure _step does not modify the tensordict\u001b[39;00m\n\u001b[0;32m--> 371\u001b[0m tensordict_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_step(tensordict)\n\u001b[1;32m    372\u001b[0m \u001b[39m# this tensordict should contain a \"next\" key\u001b[39;00m\n\u001b[1;32m    373\u001b[0m next_tensordict_out \u001b[39m=\u001b[39m tensordict_out\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnext\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[1;32m/Users/dbok/PycharmProjects/delivery-RL/delivery-RL/rl_training.ipynb Cell 26\u001b[0m in \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dbok/PycharmProjects/delivery-RL/delivery-RL/rl_training.ipynb#X34sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m assigned_c_idxs \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dbok/PycharmProjects/delivery-RL/delivery-RL/rl_training.ipynb#X34sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39mfor\u001b[39;00m o_idx, c_idx \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tensordict[\u001b[39m'\u001b[39m\u001b[39maction\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mnumpy()):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dbok/PycharmProjects/delivery-RL/delivery-RL/rl_training.ipynb#X34sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m     \u001b[39mif\u001b[39;00m c_idx \u001b[39m!=\u001b[39m \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m \\\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dbok/PycharmProjects/delivery-RL/delivery-RL/rl_training.ipynb#X34sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m tensordict[\u001b[39m'\u001b[39m\u001b[39mobservation\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmasks\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mo\u001b[39m\u001b[39m'\u001b[39m][o_idx] \\\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/dbok/PycharmProjects/delivery-RL/delivery-RL/rl_training.ipynb#X34sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m tensordict[\u001b[39m'\u001b[39;49m\u001b[39mobservation\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mmasks\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mc\u001b[39;49m\u001b[39m'\u001b[39;49m][c_idx] \\\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dbok/PycharmProjects/delivery-RL/delivery-RL/rl_training.ipynb#X34sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m         \u001b[39mand\u001b[39;00m (o_idx \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m assigned_o_idxs) \u001b[39mand\u001b[39;00m (c_idx \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m assigned_c_idxs) \\\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dbok/PycharmProjects/delivery-RL/delivery-RL/rl_training.ipynb#X34sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m     :\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dbok/PycharmProjects/delivery-RL/delivery-RL/rl_training.ipynb#X34sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m         assignment \u001b[39m=\u001b[39m (tensordict[\u001b[39m'\u001b[39m\u001b[39mobservation\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mids\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mo\u001b[39m\u001b[39m'\u001b[39m][o_idx]\u001b[39m.\u001b[39mitem(), tensordict[\u001b[39m'\u001b[39m\u001b[39mobservation\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mids\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mc\u001b[39m\u001b[39m'\u001b[39m][c_idx]\u001b[39m.\u001b[39mitem())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dbok/PycharmProjects/delivery-RL/delivery-RL/rl_training.ipynb#X34sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m         assignments\u001b[39m.\u001b[39mappend(assignment)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 100 is out of bounds for dimension 0 with size 100"
     ]
    }
   ],
   "source": [
    "for i, c in enumerate(collector):\n",
    "    if i == 1:\n",
    "        break\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.data import ReplayBuffer, LazyTensorStorage\n",
    "replay_buffer = ReplayBuffer(\n",
    "    storage=LazyTensorStorage(frames_per_batch),\n",
    "    sampler=SamplerWithoutReplacement(),\n",
    "    batch_size=sub_batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "advantage_module = GAE(\n",
    "    gamma=gamma, lmbda=lmbda, value_network=module, average_gae=True, value_key='values'\n",
    ")\n",
    "\n",
    "loss_module = ClipPPOLoss(\n",
    "    actor=policy_module_actor,\n",
    "    critic=final_module,\n",
    "    advantage_key=\"advantage\",\n",
    "    clip_epsilon=clip_epsilon,\n",
    "    entropy_bonus=bool(entropy_eps),\n",
    "    entropy_coef=entropy_eps,\n",
    "    # these keys match by default but we set this for completeness\n",
    "    value_target_key=advantage_module.value_target_key,\n",
    "    critic_coef=1.0,\n",
    "    gamma=0.99,\n",
    "    loss_critic_type=\"smooth_l1\",\n",
    ")\n",
    "\n",
    "optim = torch.optim.Adam(loss_module.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, c in enumerate(collector):\n",
    "    if i == 5:\n",
    "        break\n",
    "    advantage_module(c)\n",
    "    data_view = c.reshape(-1)\n",
    "    replay_buffer.extend(data_view.cpu())\n",
    "\n",
    "subdata = replay_buffer.sample()\n",
    "# loss_vals = loss_module(subdata)\n",
    "# loss_value = (\n",
    "#     loss_vals[\"loss_objective\"]\n",
    "#     + loss_vals[\"loss_critic\"]\n",
    "#     + loss_vals[\"loss_entropy\"]\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        action: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        advantage: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        collector.traj_ids: Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        done: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        logits: Tensor(shape=torch.Size([4, 1, 22]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        next.done: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        next.observation.ids.ar: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.int32, is_shared=False),\n",
       "        next.observation.ids.c: Tensor(shape=torch.Size([4, 21]), device=cpu, dtype=torch.int32, is_shared=False),\n",
       "        next.observation.ids.o: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.int32, is_shared=False),\n",
       "        next.observation.masks.ar: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        next.observation.masks.c: Tensor(shape=torch.Size([4, 21]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        next.observation.masks.o: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        next.observation.tensors.ar: Tensor(shape=torch.Size([4, 1, 512]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        next.observation.tensors.c: Tensor(shape=torch.Size([4, 21, 512]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        next.observation.tensors.o: Tensor(shape=torch.Size([4, 1, 512]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        next.reward: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        observation.ids.ar: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.int32, is_shared=False),\n",
       "        observation.ids.c: Tensor(shape=torch.Size([4, 21]), device=cpu, dtype=torch.int32, is_shared=False),\n",
       "        observation.ids.o: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.int32, is_shared=False),\n",
       "        observation.masks.ar: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        observation.masks.c: Tensor(shape=torch.Size([4, 21]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        observation.masks.o: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        observation.tensors.ar: Tensor(shape=torch.Size([4, 1, 512]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        observation.tensors.c: Tensor(shape=torch.Size([4, 21, 512]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        observation.tensors.o: Tensor(shape=torch.Size([4, 1, 512]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        reward: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        sample_log_prob: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        value_target: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        values: Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "    batch_size=torch.Size([4]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdata.flatten_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "advantage.shape and log_weight.shape do not match (got torch.Size([4, 1]) and torch.Size([4, 1, 1]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/dbok/PycharmProjects/delivery-RL/delivery-RL/rl_training.ipynb Cell 30\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/dbok/PycharmProjects/delivery-RL/delivery-RL/rl_training.ipynb#X61sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m loss_vals \u001b[39m=\u001b[39m loss_module(subdata)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torchrl/objectives/ppo.py:244\u001b[0m, in \u001b[0;36mClipPPOLoss.forward\u001b[0;34m(self, tensordict)\u001b[0m\n\u001b[1;32m    241\u001b[0m     batch \u001b[39m=\u001b[39m log_weight\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m    243\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m advantage\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m log_weight\u001b[39m.\u001b[39mshape:\n\u001b[0;32m--> 244\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    245\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39madvantage.shape and log_weight.shape do not match (got \u001b[39m\u001b[39m{\u001b[39;00madvantage\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mand \u001b[39m\u001b[39m{\u001b[39;00mlog_weight\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    247\u001b[0m     )\n\u001b[1;32m    248\u001b[0m gain1 \u001b[39m=\u001b[39m log_weight\u001b[39m.\u001b[39mexp() \u001b[39m*\u001b[39m advantage\n\u001b[1;32m    250\u001b[0m log_weight_clip \u001b[39m=\u001b[39m log_weight\u001b[39m.\u001b[39mclamp(\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_clip_bounds)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: advantage.shape and log_weight.shape do not match (got torch.Size([4, 1]) and torch.Size([4, 1, 1]))"
     ]
    }
   ],
   "source": [
    "loss_vals = loss_module(subdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = tqdm(total=total_frames * frame_skip)\n",
    "eval_str = \"\"\n",
    "\n",
    "# We iterate over the collector until it reaches the total number of frames it was\n",
    "# designed to collect:\n",
    "for i, tensordict_data in enumerate(collector):\n",
    "    # we now have a batch of data to work with. Let's learn something from it.\n",
    "    for _ in range(num_epochs):\n",
    "        # We'll need an \"advantage\" signal to make PPO work.\n",
    "        # We re-compute it at each epoch as its value depends on the value\n",
    "        # network which is updated in the inner loop.\n",
    "        advantage_module(tensordict_data)\n",
    "        data_view = tensordict_data.reshape(-1)\n",
    "        replay_buffer.extend(data_view.cpu())\n",
    "        for _ in range(frames_per_batch // sub_batch_size):\n",
    "            subdata = replay_buffer.sample(sub_batch_size)\n",
    "            loss_vals = loss_module(subdata.to(device))\n",
    "            loss_value = (\n",
    "                loss_vals[\"loss_objective\"]\n",
    "                + loss_vals[\"loss_critic\"]\n",
    "                + loss_vals[\"loss_entropy\"]\n",
    "            )\n",
    "\n",
    "            # Optimization: backward, grad clipping and optim step\n",
    "            loss_value.backward()\n",
    "            # this is not strictly mandatory but it's good practice to keep\n",
    "            # your gradient norm bounded\n",
    "            torch.nn.utils.clip_grad_norm_(loss_module.parameters(), max_grad_norm)\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 2\n",
    "\n",
    "class TestEnv(EnvBase):\n",
    "    def __init__(self, seed=None, device=\"cpu\"):\n",
    "        super().__init__(device=device, batch_size=[bs])\n",
    "        self._make_specs()\n",
    "        \n",
    "        if seed is None:\n",
    "            seed = torch.empty((), dtype=torch.long).random_().item()\n",
    "        self.set_seed(seed)\n",
    "        \n",
    "    def _step(self, tensordict: TensorDictBase) -> TensorDictBase:\n",
    "        '''\n",
    "        tensordict['action'] - a np.array of indexes of couriers assigned for the given order. If there is no courier assigned -1 is provided.\n",
    "        '''\n",
    "        print('next')\n",
    "        out = TensorDict(\n",
    "            {\n",
    "                \"next\": {\n",
    "                    \"observation\": TensorDict({\n",
    "                        'a': torch.tensor([[-1, -2], [-4, -5]], dtype=torch.float),\n",
    "                        'b': torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float),\n",
    "                        },\n",
    "                        batch_size=[bs]\n",
    "                    ),\n",
    "                    \"reward\": torch.tensor([0] * bs, dtype=torch.float32),\n",
    "                    \"done\": torch.tensor([False] * bs, dtype=torch.bool),\n",
    "                }\n",
    "            },\n",
    "            batch_size=[bs]\n",
    "        )\n",
    "        return out\n",
    "    \n",
    "    def _reset(self, tensordict: TensorDictBase) -> TensorDictBase:\n",
    "        if tensordict is None or tensordict.is_empty():\n",
    "            return TensorDict(\n",
    "            {\n",
    "                \"observation\": TensorDict({\n",
    "                        'a': torch.tensor([[-1, -2], [-4, -5]], dtype=torch.float),\n",
    "                        'b': torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float),\n",
    "                        },\n",
    "                        batch_size=[bs]\n",
    "                    ),\n",
    "            },\n",
    "            batch_size=[bs]\n",
    "        ) \n",
    "        return tensordict\n",
    "    \n",
    "    def _set_seed(self, seed: Optional[int]):\n",
    "        rng = torch.manual_seed(seed)\n",
    "        self.rng = rng\n",
    "\n",
    "    def _make_specs(self) -> None:\n",
    "        self.action_spec = UnboundedDiscreteTensorSpec(\n",
    "            dtype=torch.int,\n",
    "            shape=[bs]\n",
    "            # shape=(1,)\n",
    "            # shape=(self.model.tensors['o'].shape[0],)\n",
    "        )\n",
    "        observation_spec = CompositeSpec(\n",
    "            a = UnboundedContinuousTensorSpec(\n",
    "                dtype=torch.float32,\n",
    "                shape=[bs]\n",
    "            ),\n",
    "            b = UnboundedContinuousTensorSpec(\n",
    "                dtype=torch.float32,\n",
    "                shape=[bs]\n",
    "            ),\n",
    "            shape=[bs]\n",
    "        )\n",
    "        # if not isinstance(observation_spec, CompositeSpec):\n",
    "        observation_spec = CompositeSpec(observation=observation_spec, shape=[bs])\n",
    "            \n",
    "        self.observation_spec = observation_spec\n",
    "        self.reward_spec = UnboundedContinuousTensorSpec(\n",
    "            shape=[bs],\n",
    "            dtype=torch.float32,\n",
    "        )\n",
    "        self.done_spec = BinaryDiscreteTensorSpec(\n",
    "            bs,\n",
    "            shape=[bs]\n",
    "        )\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TransformedEnv(\n",
    "    TestEnv(),\n",
    "    # Compose(\n",
    "    #     # normalize observations\n",
    "    #     # ObservationNorm(in_keys=[\"observation\"]),\n",
    "    #     # DoubleToFloat(in_keys=[\"observation\"]),\n",
    "    #     StepCounter(),\n",
    "    # ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation_spec: CompositeSpec(\n",
      "    observation: CompositeSpec(\n",
      "        a: UnboundedContinuousTensorSpec(\n",
      "             shape=torch.Size([2, 3]), space=None, device=cpu, dtype=torch.float32, domain=continuous),\n",
      "        b: UnboundedContinuousTensorSpec(\n",
      "             shape=torch.Size([2, 3]), space=None, device=cpu, dtype=torch.float32, domain=continuous), device=cpu, shape=torch.Size([2, 3])), device=cpu, shape=torch.Size([2]))\n",
      "reward_spec: UnboundedContinuousTensorSpec(\n",
      "     shape=torch.Size([2]), space=None, device=cpu, dtype=torch.float32, domain=continuous)\n",
      "input_spec: CompositeSpec(\n",
      "    action: UnboundedDiscreteTensorSpec(\n",
      "         shape=torch.Size([2]), space=ContinuousBox(minimum=Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.int64, contiguous=True), maximum=Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.int64, contiguous=True)), device=cpu, dtype=torch.int32, domain=continuous), device=cpu, shape=torch.Size([2]))\n",
      "action_spec (as defined by input_spec): UnboundedDiscreteTensorSpec(\n",
      "     shape=torch.Size([2]), space=ContinuousBox(minimum=Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.int64, contiguous=True), maximum=Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.int64, contiguous=True)), device=cpu, dtype=torch.int32, domain=continuous)\n",
      "next\n",
      "next\n",
      "next\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "The dtypes of the real and fake tensordict don't match for key next.done. Got fake=torch.int64 and real=torch.bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/dbok/PycharmProjects/delivery-RL/delivery-RL/rl_training.ipynb Cell 17\u001b[0m in \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dbok/PycharmProjects/delivery-RL/delivery-RL/rl_training.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39minput_spec:\u001b[39m\u001b[39m\"\u001b[39m, env\u001b[39m.\u001b[39minput_spec)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dbok/PycharmProjects/delivery-RL/delivery-RL/rl_training.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39maction_spec (as defined by input_spec):\u001b[39m\u001b[39m\"\u001b[39m, env\u001b[39m.\u001b[39maction_spec)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/dbok/PycharmProjects/delivery-RL/delivery-RL/rl_training.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m check_env_specs(env)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torchrl/envs/utils.py:290\u001b[0m, in \u001b[0;36mcheck_env_specs\u001b[0;34m(env, return_contiguous, check_dtype, seed)\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    286\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe shapes of the real and fake tensordict don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt match for key \u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGot fake=\u001b[39m\u001b[39m{\u001b[39;00mfake_tensordict[key]\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and real=\u001b[39m\u001b[39m{\u001b[39;00mreal_tensordict[key]\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    288\u001b[0m         )\n\u001b[1;32m    289\u001b[0m     \u001b[39mif\u001b[39;00m check_dtype \u001b[39mand\u001b[39;00m (fake_tensordict[key]\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m real_tensordict[key]\u001b[39m.\u001b[39mdtype):\n\u001b[0;32m--> 290\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    291\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe dtypes of the real and fake tensordict don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt match for key \u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    292\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGot fake=\u001b[39m\u001b[39m{\u001b[39;00mfake_tensordict[key]\u001b[39m.\u001b[39mdtype\u001b[39m}\u001b[39;00m\u001b[39m and real=\u001b[39m\u001b[39m{\u001b[39;00mreal_tensordict[key]\u001b[39m.\u001b[39mdtype\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    293\u001b[0m         )\n\u001b[1;32m    295\u001b[0m \u001b[39m# test dtypes\u001b[39;00m\n\u001b[1;32m    296\u001b[0m real_tensordict \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mrollout(\u001b[39m3\u001b[39m)  \u001b[39m# keep empty structures, for example dict()\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: The dtypes of the real and fake tensordict don't match for key next.done. Got fake=torch.int64 and real=torch.bool."
     ]
    }
   ],
   "source": [
    "print(\"observation_spec:\", env.observation_spec)\n",
    "print(\"reward_spec:\", env.reward_spec)\n",
    "print(\"input_spec:\", env.input_spec)\n",
    "print(\"action_spec (as defined by input_spec):\", env.action_spec)\n",
    "\n",
    "check_env_specs(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.int32, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        next.done: Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        next.observation.a: Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        next.observation.b: Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        next.reward: Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        observation.a: Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        observation.b: Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        reward: Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "    batch_size=torch.Size([2]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n",
      "next\n",
      "next\n",
      "next\n",
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([2, 3]), device=cpu, dtype=torch.int32, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([2, 3]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        next.done: Tensor(shape=torch.Size([2, 3]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        next.observation.a: Tensor(shape=torch.Size([2, 3, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        next.observation.b: Tensor(shape=torch.Size([2, 3, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        next.reward: Tensor(shape=torch.Size([2, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        observation.a: Tensor(shape=torch.Size([2, 3, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        observation.b: Tensor(shape=torch.Size([2, 3, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        reward: Tensor(shape=torch.Size([2, 3]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "    batch_size=torch.Size([2, 3]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "fake = env.fake_tensordict().flatten_keys(\".\")\n",
    "print(fake)\n",
    "\n",
    "real_tensordict = env.rollout(3, return_contiguous=True).flatten_keys(\".\")\n",
    "print(real_tensordict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_tensordict = fake.unsqueeze(real_tensordict.batch_dims - 1).expand(*real_tensordict.shape).to_tensordict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        action: Tensor(shape=torch.Size([2, 3]), device=cpu, dtype=torch.int32, is_shared=False),\n",
       "        done: Tensor(shape=torch.Size([2, 3]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        next.done: Tensor(shape=torch.Size([2, 3]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        next.observation.a: Tensor(shape=torch.Size([2, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        next.observation.b: Tensor(shape=torch.Size([2, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        next.reward: Tensor(shape=torch.Size([2, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        observation.a: Tensor(shape=torch.Size([2, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        observation.b: Tensor(shape=torch.Size([2, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        reward: Tensor(shape=torch.Size([2, 3]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "    batch_size=torch.Size([2, 3]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# real_tensordict.apply(lambda x: torch.zeros_like(x)) == fake_tensordict.apply(lambda x: torch.zeros_like(x))\n",
    "# fake_tensordict\n",
    "fake_tensordict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.f1 = nn.Linear(3, 5)\n",
    "        self.f2 = nn.Linear(3, 5)\n",
    "\n",
    "        self.last = nn.Linear(5, 2)\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        x = self.f1(x)\n",
    "        y = self.f2(y)\n",
    "\n",
    "        return self.last(x - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7460, -0.8222]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3]], dtype=torch.float)\n",
    "y = torch.tensor([[1, 2, 3]], dtype=torch.float)\n",
    "n = Net()\n",
    "n(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_module = TensorDictModule(\n",
    "    n, in_keys=[('inp', \"x\"), ('inp', \"y\")], out_keys=[\"out\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy_module(x=x, y=y)\n",
    "inp = TensorDict(\n",
    "    {\n",
    "        'inp': TensorDict({\n",
    "            'x': x,\n",
    "            'y': y,\n",
    "        }, batch_size=())\n",
    "    },\n",
    "    batch_size=(1, 3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = policy_module(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.9481, 0.0790], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
