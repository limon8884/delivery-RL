1. Написал простенький симулятор
2. Написал сетку scoring_v1. Предсказывает скор: -eta. Принимает всю инфу (Ar, время создания и отмены). Качество не пробивало 20. Это без PointEncoderа
3. Написал сетку scoring_deom - принимает только нужную инфу. Лучше не стало.
4. Обучил отдельно PointEncoder (в связке с mlp предсказывал расстояние между точками) - обучился почти до упора.
5. Обучил с замороженным энкодером demo сетку - пробил 5.6. Можно было еще лучше, если б не забыл включить шедулер и подержал бы подольше.
6. Обучил на solving задачу - качество не растет
7. Базово перебрал гиперпараметры. Учил bs=64, SGD
8. починил лосс, получилось сходиться
9. обучил на solve задачу - аккураси 0.73



Глобально планы на стадию своего симулятора такие:
1. Supervised learning
    * обучить сетку на solve задачу до сходимости
    * посмотреть, сколько бывает каждого типа кейсов, где сеть ошибается:
        1. сеть назначила не того кандидата (но должен быть назначен кандидат)
        2. сеть назначила фейк-кандидата, а в реальности должен был быть назначен нормальный. Или наоборот.
        3. назначение на замаскированного
    * запустить симулятор с диспатчем на основе сети против диспатча дефолтного. Замерить CR.
2. Reinforcement learning
    * допилить инфру для задачи cartpole
    * запустить и отдебажиться на cartpol
    * адаптировать инфру под нашу задачу
    * обучить RL модель до сходимости